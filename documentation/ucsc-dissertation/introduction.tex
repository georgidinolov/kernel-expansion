\label{ch:introduction}

Financial markets have been the subject of research since at least
\cite{bachelier1900theorie}, and serious efforts to characterize price
variability continued through the $20^{th}$ century, with a more
modern resurgence of interest initiated by
\cite{mandelbrot1967variation}. Although long in use by market
participants in the form of heuristic rules prior to 1973
(\cite{haug2011option}), the Black-Scholes-Merton formula
\cite{black1973pricing}, \cite{merton1973theory} allowed for
statistical models of price volatility to be used in option pricing
and trading decisions. This in turn intensified the need to quantify
price dynamics and more deeply study their statistical
properties. \cite{engle1982} and \cite{bollerslev1986} introduced the
Autoregressive Conditionally Heteroscedastic (ARCH) and Generalized
Autoregressive Conditionally Heteroscedastic (GARCH) models as
state-space-like time series representation of price
volatility. Shortly thereafter followed the stochastic volatility
model of \cite{hull1987pricing}, which is still popular
today. Extensions and further applications of these models to
different assets types, markets, pricing scenarios, and inferential
settings have allowed the literature to grow tremendously. In addition
to the strides made by researchers in the field, the wide availability
of high-quality data has facilitated the study of price volatility
from a statistical standpoint. Despite their availability, two
particular types of data have not been as extensively studied in the
literature: high-frequency time series data and open, close, high, and
low (OCHL for short) data. This work develops statistical approaches
that more efficiently use these types of data.

\section{High-Frequency Data}
Traditionally, financial models have used low-frequency returns (e.g.,
daily, weekly or monthly returns) to investigate price
volatility. However, as high-frequency price data (which is roughly
defined as sampling every 5 minutes or less) has become widely
available, interest has turned to generating high-resolution estimates
of the volatility path and to improving estimates of volatility model
parameters over higher frequencies.

Attempts have been made to fit the GARCH model to high-frequency data
\cite{bollerslev1986,andersen1997intraday}.  However, the assumptions
behind the classical GARCH model are not robust with respect to the
specification of the sampling interval, and therefore the model is not
invariant to temporal aggregation \cite{drost1993aggregation},
\cite{andersen1997intraday}, \cite{zumbach2000pitfalls}.  To address
this issue some authors have turned to estimating low-frequency GARCH
and stochastic volatility models using relevant summaries of the
high-frequency prices. One popular summary statistic is the realized
variance estimator \cite{comte1998long},
\cite{andersen2001distribution}, \cite{barndorff2002estimating}.  The
realized variance is defined as the sum of squared high-frequency log
returns over the period of interest.  Under strict stationarity and
some other weak regularity conditions for the volatility process, the
realized variance converges in probability to the integrated variance
of the true diffusion process as the sampling frequency increases.
\cite{takahashi2009estimating} and \cite{shirota2014realized} use both
high-frequency returns as well as the realized variance to estimate
stochastic volatility models, while \cite{hansen2012realized} does the
same for GARCH models.  Similarly, \cite{maneesoonthorn2014inference}
use the realized volatility and the bipower variation estimators to
estimate stochastic volatility models with jumps, while
\cite{bollerslev2002estimating} use high order powers of the realized
volatility as approximations to higher orders of integrated
volatility.

A key challenge in working with high frequency prices is that they are
often contaminated with microstructure noise. Indeed, as the sampling
period shrinks down to the transaction-by-transaction frequency,
irregular spacing between transactions, discreteness in transaction
prices, and very short term temporal dependence become dominant
features of the data \cite{stoll2000presidential}.  One consequence of
the presence of microstructure noise is that the realized variance
becomes a biased and inconsistent estimator of the true integrated
variance \cite{zhou1996high}.  Possible solutions to this issue have
been proposed by \cite{zhang2005tale}, who suggest sampling data
sparsely at an optimally determined frequency and then averaging
across the possible grids over the data, \cite{ait2011ultra}, who
propose combining estimators based on subsampling data at different
frequencies, and \cite{hansen2006realized} and
\cite{barndorff2008designing}, who employ a class of kernel-based
methods similar to those used for estimating the long-run variance of
a stationary time-series in the presence of autocorrelation. In the
context of model-based approaches it is common to assume that the
summaries of the high-frequency returns used to estimate the model are
noisy versions of the true realized volatilities (e.g., see
\cite{venter2012extended, shirota2014realized}).

Chapter 1 describes a Bayesian stochastic volatility model for
high-frequency data that explicitly accounts for the presence of
microstructure noise.  Unlike other approaches in the literature, we
estimate our model directly using the high-frequency price data rather
than summaries of the high-frequency returns.  To account for the
effect of microstructure noise we introduce a hierarchical
specification in which the observed high-frequency prices are noisy
versions of the true unknown prices.  One appealing feature of our
proposed model is that it is (approximately) coherent across all
sampling frequencies, which is in line with previous efforts to
validate the application of discrete-time models for volatility in
high-frequency settings \cite{andersen1999forecasting}.  Coherency is
achieved by starting with a continuous-time model and then carefully
discretizing the exact solution to the stochastic differential
equations for the price and volatility processes, and by carefully
eliciting prior distributions for the parameters of the
continuous-time model.

\section{Bivariate OCHL Data}
Chapter 1 is concerned with high-frequency data where only the period
opening and closing prices are used to estimate volatility. However,
even for lower-frequency data, it is common to assume that asset
prices follow correlated geometric Brownian motions. In that
setting, estimates of volatility and correlation are usually based on
log-returns, which depend only on the opening and closing prices.
However, typically more information is available.  For example,
maximum and minimum trading prices over different trading intervals
are readily available which can serve as summary statistics for volatility over the periods of interest.  In univariate settings, it has been shown that
incorporating this additional information can substantially improve
estimates. For example, \cite{rodriguez2012} derive a full
likelihood-based (Bayesian) approach to estimate volatility in
univariate financial time series where open, closing, highest, and
lowest prices are included. Their work fits into a body of literature
and collection of techniques by practitioners where the observed range
of prices is used to make similar estimates. Likelihood-free
approaches, like that of \cite{rogers1991estimating},
\cite{rogers2008estimating}, on the other hand suffer from not being
able to be easily integrated into inferential frameworks that require
explicit estimates of probability, as well as being sub-optimal as
will be seen in the Simulation section below. Likelihood
approaches involving less than all four extrema have been used in
computing first passage times \cite{kou2016first, sacerdote2016first},
with application to structural models in credit risk and default
correlations \cite{haworth2008modelling, ching2014correlated}, and to
pricing financial derivative instruments whose payoff depends on some
(but not all) of the observed boundaries \cite{he1998double}.

A general likelihood-based approach for incorporating OCHL data in
bivariate model estimation has, until now, been unfeasible. This is
due to the lack of a robust and numerically efficient solution,
necessary when requiring repeated computation, to the partial
differential equation determining this likelihood. In Chapters 2 and 3
we develop this numerical solution and demonstrate the increase in
statistical power in using OCHL data when comparing to existing
methods.
