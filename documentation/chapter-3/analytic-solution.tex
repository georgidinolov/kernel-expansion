\label{ch:small-time}

% In this chapter we have established the existence of an approximate,
% analytic solution for small values of $t$ whose applicability is
% well defined in terms of error rates at the boundaries (BCs) and the
% locations of the images (ICs).
Chapter \ref{ch:galerkin} introduced a semidiscrete (finite element)
Galerkin method for solving the standardized diffusion problem
(\ref{eq:qqq}). The Galerkin method is most appropriate for moderate
to large times, since numerical errors are attenuated with increasing
diffusion time $\tilde{t}$. Further, numerical simulations
demonstrated the need to resolve the likelihood function for small
$\tilde{t}$ in order to fully use the information present in OCHL
data. We will call this parameter rage the \textit{incomputable
  region} of the Galerkin solver. For parameters in the incomputable
region, a sufficient condition to flag them as such is if the
numerical derivative of the finite element solution with respect to
the boundary parameters produces a negative value. However, this is
not a \textit{necessary} condition, and in practice this is an
insidious problem: parameter combinations which should be attributed
with a very small likelihoods may be given values orders of magnitude
higher than they should and thus bias any inferential procedure (see
Figure \ref{fig:limitations-rho-0.95-1} in Chapter \ref{ch:galerkin})
for an illustration). In this chapter, we develop an analytic solution
applicable in a small-$\tilde{t}$ region. In addition to having its
own well-defined criteria for appropriate use (namely an upper bound
on $\tilde{t}$ as well as an upper bound on $\tilde{\sigma}$). In this
chapter we also develop an analytic matching solution that bridges the
small-$\tilde{t}$ solution and the Galerkin solution across the
transient $\tilde{t}$ region. 

\section{A small-time solution to the PDE, revisited}
As part of the overall Galerkin method, Chapter \ref{ch:galerkin}
introduced a small-time approximation for the normalized diffusion
problem \eqref{eq:qqq} via the method of images (see Section
\ref{sec:pde-small-t}). By reflecting the fundamental solution
(i.e. the solution to the governing PDE without the boundary conditions) about the
closest boundary and picking a sufficiently small
$\tilde{t}_\epsilon$, the sum of images function \eqref{eq:p-epsilon}
\begin{align}
  p_\epsilon(\tilde{x}, \tilde{y}, \tilde{t}) &= G(\tilde{x}, \tilde{y}, \tilde{t} | \tilde{x}_0, \tilde{y}_0) - G(\tilde{x}, \tilde{y}, \tilde{t} | \tilde{x}'_0, \tilde{y}'_0)
\end{align}
satisfies the initial condition, the governing PDE, and the boundary
conditions. However, because the analytic dependence of this
small-time approximation on the boundaries is only through the
location parameters $(\tilde{x}'_0, \tilde{y}'_0)$ of the single
reflected image, differentiating with respect to all four boundaries
yields a uniform zero value in computing the transition density
$\frac{\partial^4}{\partial a_x \partial b_x \partial a_y \partial
  b_y} p(\tilde{x}, \tilde{y}, \tilde{t})$.

\subsection{Uniqueness and Symmetry Condition}
The insufficiency of the previous small-time solution suggests
extending the system of images by performing more than a single
reflection. If there exists an image whose location is the
result of at least one reflection about each of the four boundaries,
then this image is guaranteed to have a non-trivial contribution to
the density
$\frac{\partial^4}{\partial a_x \partial b_x \partial a_y \partial
  b_y} p(\tilde{x}, \tilde{y}, \tilde{t})$. An immediate problem with
the extension, however, is the uniqueness of the resultant approximate
density function. To illustrate the problem, consider the two systems
of images constructed by carrying out the series of reflections
\begin{align}
  & R_1 := \left\{ 1,2,3,4 \right\}, & R_2 := \left\{ 2,3,4,1 \right\}, & \label{eq:reflection-sets}
\end{align}
where $\left\{ r_i, r_j, r_k, r_l \right\}$ denotes the set of images
generated by the following steps
\begin{enumerate}
\item $r_i:$ Reflect the initial condition
  $(\tilde{x}_0, \tilde{y}_0)$ about boundary $r_i$ to produce
  coordinate $(\tilde{x}_{r_i}, \tilde{y}_{r_i})$. The system of
  images consists of locations
  \begin{align*}
    \left\{(\tilde{x}_0, \tilde{y}_0), (\tilde{x}_{r_i},
  \tilde{y}_{r_i})\right\}
  \end{align*}
  with signs $\left\{1, -1\right\}$.
\item $r_j:$ Reflect each of $\left\{(\tilde{x}_0, \tilde{y}_0), (\tilde{x}_{r_i},
  \tilde{y}_{r_i})\right\}$ about boundary $r_j$ and add to existing set of images to produce
  \begin{align*}
    \left\{ (\tilde{x}_0, \tilde{y}_0), (\tilde{x}_{r_i},
    \tilde{y}_{r_i}), (\tilde{x}_{r_j}, \tilde{y}_{r_j}), (\tilde{x}_{r_j, r_i},
    \tilde{y}_{r_j, r_i}) \right\}
  \end{align*}
  with signs $\left\{1,-1,-1,1\right\}$.

\item $r_k:$ Reflect each of $\left\{ (\tilde{x}_0, \tilde{y}_0), (\tilde{x}_{r_i},
    \tilde{y}_{r_i}), (\tilde{x}_{r_j}, \tilde{y}_{r_j}), (\tilde{x}_{r_j, r_i},
    \tilde{y}_{r_j, r_i}) \right\}$ about boundary $r_k$ and add to existing set of images to produce
  \begin{align*}
    & \left\{ (\tilde{x}_0, \tilde{y}_0), (\tilde{x}_{r_i},
    \tilde{y}_{r_i}), (\tilde{x}_{r_j}, \tilde{y}_{r_j}), (\tilde{x}_{r_j, r_i},
    \tilde{y}_{r_j, r_i}), \right. & \\
    & \left. (\tilde{x}_{r_k}, \tilde{y}_{r_k}), (\tilde{x}_{r_k, r_i},
    \tilde{y}_{r_k, r_i}), (\tilde{x}_{r_k, r_j}, \tilde{y}_{r_k, r_j}), (\tilde{x}_{r_k, r_j, r_i},
    \tilde{y}_{r_k, r_j, r_i})  \right\}&
  \end{align*}
  with signs $\left\{1,-1,-1,1, -1,1,1,-1\right\}$.
\item $r_l:$ Reflect each of the existing image locations about
  boundary $r_l$ and add to the existing set of images.
\end{enumerate}
Here, boundary $1$ corresponds to $\tilde{y}=0$, boundary 2 to
$\tilde{x}=1$, boundary 3 to $\tilde{y}=1$, and boundary 4 to
$\tilde{x} = 0$.  Given the set $\left\{r_i,r_j,r_k,r_l\right\}$, we
re-define $p_\epsilon$ as the the sum of all $J = 16$ images
\begin{equation*}
  p_\epsilon(\tilde{x}, \tilde{y}, \tilde{t}) = \sum_{j=1}^J (-1)^{n(j)}
  G(\tilde{x},\tilde{y},\tilde{t}|\tilde{x}_{(j)},\tilde{y}_{(j)}),
\end{equation*}
where $n(j)$ is the number of reflections performed to produce image
$j$, and $(\tilde{x}_{(j)},\tilde{y}_{(j)})$ is the $j^{th}$ image
location in the sequence at the end of Step 4 above. The newly defined
$p_\epsilon$ analytically satisfies the boundary condition at $r_l$
(since it is the last reflection), and it also satisfies the PDE. Once
again we can choose a sufficiently small $\tilde{t}_\epsilon$ such
that the boundary conditions at $r_i, r_j,$ and $r_k$ hold numerically
as well.

Referring back to the example reflection sets in
\eqref{eq:reflection-sets}, although not identical, the two solutions
to the PDE problem corresponding to $R_1$ and $R_2$ have minimal
relative differences. From a numerical implementation standpoint, there is only
near machine-$\varepsilon$ difference between them. However, the two
solutions have a single image,
$(\tilde{x}_{1,2,3,4}, \tilde{y}_{1,2,3,4})$ and
$(\tilde{x}_{2,3,4,1}, \tilde{y}_{2,3,4,1})$ respectively, whose
location is a function of all four boundaries, and this single image
entirely defines the corresponding density function
$\frac{\partial^4}{\partial a_x \partial b_x \partial a_y
  \partial b_y}p_\epsilon$. Because the location parameters of these images are
different, the joint densities of $(x_0, y_0, x, y, a_x, b_x, a_y, b_y)$
derived from the two PDE solutions are consequently very different as
well. Uniqueness of the solution using the method of images is
achieved by performing an infinite number of reflections, which is not
always possible (see Section \ref{sec:proof} below).

A condition weaker than uniqueness which nonetheless restricts the
solution space is the symmetry obeyed by the problem. We consider the transformation
\begin{align}
  \label{eq:symmetry-condition}
  \begin{split}
    x^{new} = (a_x + b_x) - x^{old}, \\
    y^{new} = (a_y + b_y) - y^{old}.
  \end{split}
\end{align}
The PDF solution to the initial-boundary problem of the diffusion
equation and joint density of $(x_0,y_0,x,y,a_x,b_x,a_y,b_y)$ are
invariant with respect to this transformation. Under the normalized
problem, when $(\tilde{x}_0, \tilde{y}_0) = (1/2,\,1/2)$ and $\rho=0$,
the proposed system of images must map to itself under the
corresponding coordinate transformation. Further, if we require that
the system of images contain the fewest possible elements, then the
unique system of images is the union of the sets of reflections
\begin{align}
  \left\{ 2,4,1,3 \right\} \cup \left\{ 2,4,3,1 \right\} \cup \left\{ 4,2,1,3 \right\} \cup \left\{ 4,2,3,1 \right\}. \label{eq:union}
\end{align}
When $(\tilde{x}_0, \tilde{y}_0) = (1/2,1/2)$ and $\rho=0$, the set of
images from \eqref{eq:union} is closed under the transformation as
desired:
\begin{align*}
  \{2,4,1,3 \} &\to \{4,2,3,1 \}, \\
  \{2,4,3,1 \} &\to \{4,2,1,3 \}, \\
  \{4,2,1,3 \} &\to \{2,4,3,1 \}, \\
  \{4,2,3,1 \} &\to \{2,4,1,3 \}.
\end{align*}
This is illustrated in Figure \ref{fig:illustration-1}. Removing
duplicate member images of \eqref{eq:union} and summing over the
remaining $J^* \leq 64$, we define the new small-time solution
\begin{equation}
  p_\epsilon(\tilde{x}, \tilde{y}, \tilde{t}) = \sum_{j=1}^{J^*} (-1)^{n(j)}
  G(\tilde{x},\tilde{y},\tilde{t}|\tilde{x}_{(j)},\tilde{y}_{(j)}). \label{eq:small-time-sol}
\end{equation}
We can control the relative error of the approximation by setting the
minimum absolute value of $p_\epsilon$ at the boundaries and solving
for $\tilde{t}_\epsilon$.
% For a boundary error rate of at most $1 \cdot 10^{-12}$, the
% solution based on the distribution bound by the symmetric (green)
% images from Figure (\ref{fig:illustration-1}) produces palatable
% relative errors across $\Omega$ compared to the true closed-form
% solution, as shown in Figure (\ref{fig:illustration-rel-error}) where
% the maximum relative error on a $30 \times 30$ grid is $O(10^{-5})$.
% Decreasing the error rate on the boundary to $1 \cdot 10^{-14}$ forces
% $t^* \approx 0.047$ and the max relative error on the same grid is
% $O(10^{-6})$.
%
% As in Chapter 2, we can scale,
% rotate, and scale again the problem, so that the boundaries of the
% computational domain have changed, but the fundamental solution to the
% problem is defined by an uncorrelated bivariate Gaussian distribution
% with variance $t$ in each direction. The transformed problem is shown
% in Figure (\ref{fig:transformed-problem-rho-0}).
% The topology under the transformed problem (under $T_{(3)}$ in
% \eqref{eq:T3}) permits the repeated reflection of the fundamental
% solution about the boundaries without any of the images falling within
% the computational domain $\Omega_3$. This further allows for
% subsequent images for fall further from the domain (scaling linearly
% in distance) so that there exists an asymptotic series composed of an
% infinite number of reflections which enforce the IC/BCs and satisfy
% the governing PDE.

% \begin{figure}
%   \centering
%   \includegraphics[width=1\linewidth]{../chapter-3/illustration-rho-0-normalized.pdf}
%   \caption{The computational domain $\Omega_2$ for the normalized
%     problem (\ref{eq:qqq}) is the unit square centered on
%     $(0.5,0.5)$, shown in the figure as a square box. Level sets
%     of the fundamental solution with $\rho = 0$ and
%     $\sigma_{\tilde{y}} < 1$ are also shown at an initial
%     condition in the upper-left corner of $\Omega_2$.}
%   \label{fig:normalized-problem-rho-0}
% \end{figure}

% \begin{figure}
%   \centering
%   \includegraphics[width=1\linewidth]{../chapter-3/illustration-rho-0-transformed.pdf}
%   \caption{The problem in Figure
%     \ref{fig:normalized-problem-rho-0} is scaled by
%     $1/\tilde{\sigma}$ in the $y-$direction, then rotated $\pi/4$
%     counter-clockwise, then scaled again in the principal axes
%     direction in order to eliminate the mixing term in the
%     resultant PDE problem. Since $\rho=0$ here, the final scaling
%     is the identity transformation and the boundaries remain
%     orthogonal. The fundamental solution is symmetric under
%     rotations centered on the initial condition under the new topology, as shown
%     here.}
%   \label{fig:transformed-problem-rho-0}
% \end{figure}

% We can compose such a finite system of images by reflecting about each
% of the boundaries a number of times and picking the greatest possible
% time $t^{*}$ such that all boundaries are either numerically or
% analytically enforced. One candidate for such a set of reflections is
% shown in Figure (\ref{fig:illustration-1}), where the set of images is
% produced by the reflections (left, right, left, top, bottom, top). The
% solid colored points are image positions having non-zero derivatives
% with respect to all four boundary parameters. The green points bound a
% symmetric solution respect to a family of transformations centered the
% IC (red).

\begin{figure}
  \begin{tabular}{c}
    \begin{minipage}{0.99\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/figures/chapter-3-figure-illustration-1.pdf}
      \caption{The finite system of images resultant from the sequence
        of reflections in the set \eqref{eq:union}, for
        $(\tilde{x}_0,\tilde{y}_0) = (1/2, 1/2)$ and $\rho=0$. All
        points outside $\tilde{\Omega}$ are image position resulting
        from the reflections, where the solid green points have
        positions dependent on $(\tilde{x}_0,\tilde{y}_0)$ \textit{as
          well as all of the boundaries}, and only they contribute to
        the likelihood solution as given in \eqref{eq:derivs-1}. The
        green colored points are symmetric about the initial condition
        with respect to horizontal and vertical reflections centered
        on $(\tilde{x}_0, \tilde{y}_0)$ and hence are closed under the
        transformation \eqref{eq:symmetry-condition}. Moreover, this
        configuration is unique under the symmetry and minimal number
        of images conditions.}
      \label{fig:illustration-1}
    \end{minipage}
  \end{tabular}
\end{figure}
%%
% \begin{figure}
%   \begin{tabular}{c}
%     \begin{minipage}{0.99\textwidth}
%       \centering
%       \includegraphics[width=1\linewidth]{../chapter-3/chapter-3-figure-illustration-rel-error.pdf}
%       \caption{Relative error across $\Omega$ on a $30 \times 30$
%         regular grid based on solution (\ref{eq:small-time-sol}) for
%         the symmetric distribution (green set) of images in Figure
%         \ref{fig:illustration-1}. The max admissible small time
%         $t^*$ is bounded above by $0.058$ and used here. The relative
%         error rate is on the order of $0.01\%$}
%       \label{fig:illustration-rel-error}
%     \end{minipage}
%   \end{tabular}
% \end{figure}


\section{Calculation of the Joint Density}
\begin{figure}
  \begin{tabular}{c}
    \begin{minipage}{1.00\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/figures/chapter-3-figure-illustration-likelihood-profile.pdf}
      \caption{Log-likelihood profile comparison between small-time
        truncated solution (as given in (\ref{eq:truncated-approx}))
        and the true analytic solution for $\rho=0$. The time up to
        which the untruncated small-time solution is valid is shown
        with the solid, vertical red line. We see that in this example
        the truncated approximation deteriorates for
        $\mathcal{O}(t) = 1$, well after the technical bound set by
        $t^*$.}
      \label{fig:illustration-likelihood-profile}
    \end{minipage}
  \end{tabular}
\end{figure}

\begin{figure}
  \centering
  \begin{tabular}{c}
    \begin{minipage}{0.90\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/chapter-3-figure-illustration-rel-error-likelihood.pdf}
      \caption{Relative error for the likelihood problem comparing the
        untruncated solution to the truncated small-time approximation in
        (\ref{eq:truncated-approx}) on a $30 \times 30$ regular grid
        over $\Omega$. }
      \label{fig:illustration-rel-error-likelihood}
    \end{minipage}
  \end{tabular}
  % %% 
  % & \begin{minipage}{0.40\textwidth}
  %   \centering
  %   \includegraphics[width=1\linewidth]{chapter-3-figure-illustration-rel-error-likelihood.pdf}
  %   \caption{}
  %   \label{fig:illustration-rel-error-likelihood-truncated}
  % \end{minipage}
  % \end{tabular}
\end{figure}
  
Out of the $J^*$ images in \eqref{eq:small-time-sol}, exactly four
have location parameters dependent on all of the boundary parameters
$(a_x, b_x, a_y, b_y)$. Hence, the density calculation with the
small-time solution becomes
\begin{equation}
  \frac{\partial^4 p_\epsilon(\tilde{x}, \tilde{y}, \tilde{t})}{\partial a_x \partial b_x \partial a_y \partial b_y}  = \sum_{j'=1}^{4}
                                                                                                                        \frac{\partial^4G(\tilde{x},\tilde{y},\tilde{t}|\tilde{x}_{(j')},\tilde{y}_{(j')})}{\partial a_x \partial b_x \partial a_y \partial b_y}. \label{eq:derivs-1}
\end{equation}
One approach to compute the derivatives in (\ref{eq:derivs-1}) is to
numerically perturb the boundary parameters $(a_x,b_x,a_y,b_y)$ and
use a finite difference approximation directly on the sum
(\ref{eq:small-time-sol}). However, since the small-time solution
generally requires the use of a small time
$\tilde{t} \leq \tilde{t}_\epsilon \ll 1$, numerical underflow makes
direct application of finite difference hopeless.  We can, however,
leverage the very tractable analytic form of the Gaussian kernel
$G(\cdot)$ in \eqref{eq:Gfundamental} by defining the following
factors to more easily express the higher-order derivatives applied to
(\ref{eq:derivs-1})
\begin{align}
  \mathcal{C}(\tilde{t}, \sigma_{\tilde{y}}, \rho) &:= -\frac{1}{2\,\,\tilde{t}\, \sigma_{\tilde{y}}^2 (1-\rho^2)}, \\
  \mathcal{P}_j(\tilde{x},\tilde{y} | \tilde{x}_{(j)}, \tilde{y}_{(j)}) &:= \left(\tilde{x}- \tilde{x}_{(j)}\right)^2 \sigma_{\tilde{y}}^2 + \left(\tilde{y}-\tilde{y}_{(j)}\right)^2 - 2\rho(\tilde{x}-\tilde{x}_{(j)})(\tilde{y}-\tilde{y}_{(j)})\sigma_{\tilde{y}}.
\end{align}
The two key observations here are that $\mathcal{P}_j(\cdot)$ is
independent of $\tilde{t}$, meaning $\mathcal{C}(\cdot)$ carries the
$\tilde{t}$ dependency in differentiation and that only
$\mathcal{P}_j$ is dependent on the boundary parameters through
$\tilde{x}_{(j)}$ and $\tilde{y}_{(j)}$. Without loss of generality,
we can express the derivatives:
\begin{align}
  % \frac{\partial}{\partial a_x} G(x,y|t^{*}, \tilde{\sigma}, \rho, x_0^{(j^*)}, y_0^{(j^*)}) &= G \cdot \mathcal{C}\,\,\cdot \left( \frac{\partial \mathcal{P}}{\partial a_x} \right)\\
  % \frac{\partial^2}{\partial a_x \partial b_x} G(x,y|t^{*}, \tilde{\sigma}, \rho, x_0^{(j^*)}, y_0^{(j^*)}) &= G \cdot \mathcal{C}^2 \left( \frac{\partial \mathcal{P}}{\partial b_x} \right) \left( \frac{\partial \mathcal{P}}{\partial a_x} \right) + G \cdot \mathcal{C} \cdot \left( \frac{\partial^2 \mathcal{P}}{\partial a_x \partial b_x} \right) \\
  % %% %% %%
  % \frac{\partial^3}{\partial a_x \partial b_x \partial a_y} G(x,y|t^{*}, \tilde{\sigma}, \rho, x_0^{(j^*)}, y_0^{(j^*)}) &= G \cdot \mathcal{C}^3 \left( \frac{\partial \mathcal{P}}{\partial b_x} \right) \left( \frac{\partial \mathcal{P}}{\partial a_x} \right) \left( \frac{\partial \mathcal{P}}{\partial a_y} \right) \nonumber \\
  %                                                                                            &\,\, + G \cdot \mathcal{C}^2 \left[\left( \frac{\partial^2 \mathcal{P}}{\partial a_x \partial a_y} \right) \left( \frac{\partial \mathcal{P}}{\partial b_x} \right) + \left( \frac{\partial^2 \mathcal{P}}{\partial b_x \partial a_y} \right) \left( \frac{\partial \mathcal{P}}{\partial a_x} \right) + \left( \frac{\partial^2 \mathcal{P}}{\partial a_x \partial b_x} \right) \left( \frac{\partial \mathcal{P}}{\partial a_y} \right)\right] \nonumber \\
  %                                                                                            &\,\, + G \cdot \mathcal{C} \left( \frac{\partial^3 \mathcal{P}}{\partial a_x \partial b_x \partial a_y} \right) \nonumber \\
  %                                                                                            %% %% %%
  \frac{\partial^4}{\partial a_x \partial b_x \partial a_y \partial b_y} G(\tilde{x},\tilde{y},\tilde{t} |  \tilde{x}_{(j)}, \tilde{y}_{(j)}) &= \\
  \MoveEqLeft[10] G \cdot \mathcal{C}^4 \cdot \left(\partial_{a_x}\partial_{b_x} \partial_{a_y}\partial_{b_y} \right)\mathcal{P}&  \nonumber \\
  \MoveEqLeft[10] \,\, + G \cdot \mathcal{C}^3 \cdot \left( \partial^2_{a_x\, b_x} \partial_{a_y} \partial_{b_y} + \partial^2_{a_x\, a_y} \partial_{b_x} \partial_{b_y} + \partial^2_{a_x\, b_y} \partial_{b_x} \partial_{a_y} + \right. &\nonumber \\
  \MoveEqLeft[10] \left. \qquad\qquad\qquad +\partial^2_{b_x\, a_y} \partial_{a_x} \partial_{b_y} + \partial^2_{b_x\, b_y} \partial_{a_x} \partial_{a_y} \partial^2_{a_y\, b_y} \partial_{a_x} \partial_{b_x} \right) \mathcal{P} &  \nonumber \\
  \MoveEqLeft[10] \,\, + G \cdot \mathcal{C}^2 \cdot \left( \partial^3_{a_x\,b_x\,a_y} \partial_{b_y} + \partial^2_{a_x\,b_x}\partial^2_{a_y\,b_y} + \partial^3_{a_x\,b_x\,b_y}\partial_{a_y}  + \right.& \nonumber \\
  \MoveEqLeft[10] \left. \qquad\qquad\qquad + \partial^3_{a_x\,a_y\,b_y} \partial_{b_x} + \partial^2_{a_x\,a_y} \partial^2_{b_y\,b_x} + \partial^3_{b_x\,a_y\,b_y}\partial_{a_x} + \partial^2_{b_x\,a_y}\partial_{a_x}\partial_{b_y} \right) \mathcal{P} & \nonumber \\
  \MoveEqLeft[10] \,\, + G \cdot \mathcal{C} \cdot \partial^4_{a_x\,b_x\,a_y\,b_y} P,& \label{eq:fourth-deriv}
\end{align}
where $\partial_{x}G := \partial G/\partial x\,$,
$\partial^2_{x\,y}G := \partial^2 G/\partial x \partial y,$
$\partial_{x}\partial_{y}G := \partial G/\partial x \cdot \partial
G/\partial y,$ etc.  With (\ref{eq:fourth-deriv}), we can express each
of the derivatives in (\ref{eq:derivs-1}) as the sum of derivatives of
the polynomial $\mathcal{P}_j$ with respect to the boundary
parameters. In particular, the derivatives in (\ref{eq:fourth-deriv})
avoid the previous numerical underflow problems and lend themselves to
finite difference approximation even at high orders.

Thinking of $\tilde{t}$ as variable allows us to further simplify
(\ref{eq:fourth-deriv}). Since
$\mathcal{C} = \mathcal{O}(1/\tilde{t})$, all three terms
$G\cdot \mathcal{C}^3, G\cdot \mathcal{C}^2,$ and $G\cdot \mathcal{C}$
are
$o\left( G\cdot \mathcal{C}^4 \cdot \left(\partial_{a_x}\partial_{b_x}
    \partial_{a_y}\partial_{b_y} \right)\mathcal{P} \right)$, so that
the $G\cdot \mathcal{C}^4$ order term in (\ref{eq:fourth-deriv})
dominates the others for sufficiently small $\,\,\tilde{t}$. This
truncation, when appropriate, is useful for reducing computational
complexity \textit{and} it guarantees that the numerical
implementation of the derivatives produces positive values for all
$\tilde{t}$ as long as
$\left(\partial_{a_x}\partial_{b_x} \partial_{a_y}\partial_{b_y}
\right)\mathcal{P}$ is positive. Truncating at $\mathcal{C}^4$, the
analytic likelihood approximation given by the sum of images is of the
form:
\begin{align}
  \frac{\partial^4 p_\epsilon(\tilde{x}, \tilde{y}, \tilde{t})}{\partial a_x
  \partial b_x \partial a_y \partial b_y} &\approx \sum_{j'=1}^{4} G(\tilde{x},\tilde{y},\tilde{t}|\tilde{x}_{(j')},\tilde{y}_{(j')}) \cdot \mathcal{C}^4 \cdot \left(\partial_{a_x}\partial_{b_x} \partial_{a_y}\partial_{b_y} \right)\mathcal{P}_{j'}. \label{eq:truncated-approx}
  % &\sum_{j} \left\{ \frac{1}{2\pi\,\, t\tilde{\sigma}\sqrt{1-\rho^2}} \exp\left(
  %   -\frac{1}{2\,\,t^*\, \tilde{\sigma}^2 (1-\rho^2)} \left[
  %     \left(x-x_0^{(j)}\right)^2 \tilde{\sigma}^2 +
  %     \left(y-y_0^{(j)}\right)^2 -
  %     2\rho(x-x_0^{(j)})(y-y_0^{(j)})\tilde{\sigma} \right]\right)
  %   \cdot \right. & \nonumber \\
  % &\qquad \qquad \left. \left( -\frac{1}{2\,\,t^*\, \tilde{\sigma}^2 (1-\rho^2)} \right)^4 \cdot \left(\partial_{a_x}\partial_{b_x}
  %   \partial_{a_y}\partial_{b_y} \right)\mathcal{P} \right\}. & \nonumber
  % % & \propto IG\left( t^* | \alpha = 4, \beta = -\frac{1}{2\, \tilde{\sigma}^2 (1-\rho^2)} \left[
  % %     \left(x-x_0^{(j)}\right)^2 \tilde{\sigma}^2 +
  % %     \left(y-y_0^{(j)}\right)^2 -
  % %     2\rho(x-x_0^{(j)})(y-y_0^{(j)})\tilde{\sigma} \right] \right)
\end{align}

The above approximate solution is close to the untruncated (i.e. the
one that includes all the term
$\mathcal{C}^4, \mathcal{C}^3, \mathcal{C}^2,$ and $\mathcal{C}^1$
term solution) small-time solution for
$\tilde{t} \leq \tilde{t}_\epsilon$. Extending beyond
$\tilde{t}_\epsilon$ produces a positive function dominated by the
hyperbolic term $\mathcal{C}^4$. Figure
\ref{fig:illustration-likelihood-profile} shows a profile comparison
of the log-likelihood with respect to $\tilde{t}$ for a representative
sample configuration and $\rho=0$. The truncated approximate solution
tracks well with the true analytic solution up to
$\mathcal{O}(\tilde{t}) = 1$. Applying a first-order finite difference
approximation with a finite step $\Delta = 10^{-5}$ to compute the
polynomial derivatives produces relative errors with a maximum of
order $\mathcal{O}(10^{-4})$ on a $30 \times 30$ grid shown in Figure
\ref{fig:illustration-rel-error-likelihood}.





% An empirical
% validation is discussed below (see Figures (\ref{fig:modes-histogram})
% - (\ref{fig:modes-histogram-3})), which provide views of the errors in
% approximating the true value $\tilde{t}$ maximizing the likelihood and
% the approximation derived from the truncated small-time solution:
% \[ \min_{j'} \{ \beta_{j'}/(\alpha_{j'}+1) \} - \tilde{t}_{\max},
% \] where $\tilde{t}_{\max}$ is the $\tilde{t}$ value maximizing the
% true likelihood (for $\rho=0$) given some parameter combination
% $(\tilde{x}_0, \tilde{y}_0, \tilde{x}_{\tilde{t}},
% \tilde{y}_{\tilde{t}})$ and $\sigma_{\tilde{y}}$. Each data point in
% the histograms/scatterplots corresponding to such a combination is
% generated by sampling
% \begin{align*} \sigma_x &\sim \mbox{Lognormal}(2, 2), & \sigma_y &\sim
%   \mbox{Lognormal}(2, 2), & \rho &= 0,
% \end{align*} then simulating from a bivariate Brownian process using a
% forward Euler method and a small time step. Normalizing to the
% standard diffusion problem (see equation
% (\ref{eq:qqq})), $\tilde{t}_{\max}$ is found numerically
% conditional on $(\tilde{x}_0, \tilde{y}_0, \tilde{x}_{\tilde{t}},
% \tilde{y}_{\tilde{t}})$ and $\sigma_{\tilde{y}}$, since we
% have the closed-form likelihood for the $\rho=0$ case. For the same
% input parameters, we generate four images as given by our method above
% and then find $\min_{j'} \{ \beta_{j'}/(\alpha_{j'}+1) \}$. We compare how
% the Galerkin solution performs at this boundary of the computable
% region and beyond in the following figures.
% The histogram in Figure (\ref{fig:modes-histogram}) shows the
% distribution of signs and magnitudes of the error differences in
% approximating the modes
% $\tilde{t}_{\mbox{max}} - \min_{j'} \{ \beta_{j'}/(\alpha_{j'}+1) \}$,
% which are correlated with small $\sigma_{\tilde{y}}$ (Figure
% (\ref{fig:modes-scatterplot})). Further, can see from the plot of true
% mode vs. error in Figure (\ref{fig:modes-scatterplot-2}) that largest
% errors occur for modes that are large on relative to the
% $\tilde{t}-$scale. The distribution of true log-likelihood values at
% the approximate modes is given in Figure
% (\ref{fig:modes-histogram-2}). We can see that these function values
% are relatively large and should be resolvable by the finite element
% solver. This is confirmed in Figure
% (\ref{fig:modes-histogram-3}), which shows the relative error using
% the Galerkin solver.
% \begin{figure}
%   \begin{tabular}{cc}
%     \begin{minipage}{0.50\textwidth}
%       \centering
%       \includegraphics[width=1\linewidth]{../chapter-3/chapter-3-figure-validation-modes-histogram.pdf}
%       \caption{A histogram of differences
%         $t_{\max} - \min_{j'} \{ \beta_{j'}/(\alpha_{j'}+1) \}$ between the
%         approximate mode of the truncated small-time solution and the
%         true mode of closed-form solution, when $\rho=0$. We see that
%         the order magnitude of error in approximating the true point of
%         maximum for the likelihood function is 0.1, where the bias is
%         towards underestimating the mode.}
%       \label{fig:modes-histogram}
%     \end{minipage} &
%     \begin{minipage}{0.50\textwidth}
%       \centering
%       \includegraphics[width=1\linewidth]{../chapter-3/chapter-3-figure-validation-modes-scatterplot.pdf}
%       \caption{Scatterplot of $\tilde{\sigma}$ vs.
%         $\min_{j'} \{ \beta_{j'}/(\alpha_{j'}+1) \} -
%         t_{\max}$. Larger approximation errors in the mode point are
%         correlated with smaller $\sigma_{\tilde{y}}$.}
%       \label{fig:modes-scatterplot}
%     \end{minipage}
%   \end{tabular}
% \end{figure}
% \begin{figure}
%   \begin{tabular}{cc}
%     \begin{minipage}{0.50\textwidth}
%       \centering
%       \includegraphics[width=1\linewidth]{../chapter-3/chapter-3-figure-validation-modes-scatterplot-2.pdf}
%       \caption{Scatterplot of true mode value vs.
%         $\min_{j'} \{ \beta_{j'}/(\alpha_{j'}+1) \} - \tilde{t}_{\max}$. Larger
%         approximation errors in the mode point are correlated with
%         higher mode locations.}
%       \label{fig:modes-scatterplot-2}
%     \end{minipage} &
%     \begin{minipage}{0.50\textwidth}
%       \centering
%       \includegraphics[width=1\linewidth]{../chapter-3/chapter-3-figure-validation-modes-histogram-2.pdf}
%       \caption{Histogram of log-likelihood values at approximate mode
%         locations.  We can see that, with respect to machine
%         $\epsilon$ the true function values at the approximate modes
%         are large and ought to be resolved by the finite element
%         solver.}
%       \label{fig:modes-histogram-2}
%     \end{minipage}
%   \end{tabular}
% \end{figure}
% %
% \begin{figure}
%   \begin{tabular}{c}
%     \begin{minipage}{0.90\textwidth}
%       \centering
%       \includegraphics[width=1\linewidth]{../chapter-3/chapter-3-figure-validation-modes-histogram-3.pdf}
%       \caption{Histogram of relative error of FEM solver at the
%         approximate modes.}
%       \label{fig:modes-histogram-3}
%     \end{minipage}
%   \end{tabular}
% \end{figure}

% \begin{figure}
%   \centering
%   %%
%   %%
%   \includegraphics[scale=0.8]{illustration-rho-0-all-configurations.pdf}
%   \caption{All 24 non-unique systems of images that result from
%     reflecting about each of the boundaries once. Blue dots represent
%     location parameters of images whose weight is positive, while
%     green dots represent locations of images with negative
%     weights. The red dot is the image location that is a function of
%     all four boundaries.}
%   \label{fig:all-configurations}
% \end{figure}





\section{Existence of valid systems of images} \label{sec:proof}
\begin{figure}[h!]
  \begin{tabular}{c}
    \begin{minipage}{0.99\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/chapter-3-figure-counterexample-1.pdf}
      \caption{When $\rho=0.9$, the system of images generated by the
        set of reflections in \eqref{eq:union} and
        $(\tilde{x}_0, \tilde{y}_0) = (0.1, 0.3)$ now violates the
        initial condition as at least one of the images falls within
        $\tilde{\Omega}$. }
      \label{fig:counterexample-1}
    \end{minipage}
  \end{tabular}
\end{figure}

For $\rho=0$, the above procedure is guaranteed to produce an
admissible set of images regardless of the initial condition and
$\sigma_{\tilde{y}}$ used. However, this is not the case for a general
combination of $\rho$ and $\sigma_{\tilde{y}}$. Consider Figure
\ref{fig:counterexample-1}, which features an initial condition
$(\tilde{x}_0, \tilde{y}_0)=(0.1,0.3)$ with $\rho=0.9$. We can see
that applying the above procedure produces a system with images within
the computational domain, violating the initial condition.  However,
below we prove that for any $(\tilde{x}_0, \tilde{y}_0)$ and $\rho$
combination, we can find a sufficiently small $\sigma_{\tilde{y}}$
such that a series of reflections used for $p_\epsilon$ does not
violate the problem.

%%
\begin{figure}
  \begin{tabular}{c}
    \begin{minipage}{0.99\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/chapter-3-figure-proof-1.pdf}
      \caption{Geometry characteristic of the transformed problem when
        $\rho>0$. The thick dashed line defines the axis along which
        images resultant from reflecting about boundaries 2 and 4
        fall. The blue point denotes the intersection of the axis and
        the extension of boundary 3. When reflect with respect to
        boundary 1, for example, some of the images shown will fall
        into region $\Omega$.}
      \label{fig:proof-1}
    \end{minipage}
  \end{tabular}
\end{figure}

\begin{lemma}
  Given $\rho > 0$, and
  $(\tilde{x}_0, \tilde{y}_0) \in \tilde{\Omega}$, there exists
  $0 < \sigma_{\tilde{y}} \leq 1$ such that
  $(\tilde{x}_0^{(j)}, \tilde{y}_0^{(j)}) \notin \tilde{\Omega},
  \forall \, j\in \left\{1, \ldots, J\right\}$, where the collection
  of image locations $\left\{1, \ldots, J\right\}$ is the result of a
  finite set of reflections about the boundaries of $\tilde{\Omega}$.
\end{lemma}

\begin{proof}
  Given that in the normalized problem $\tilde{\Omega}$ is a unit
  square whose lower-left corner is centered on the origin, applying
  the transformation $T_{(3)}$ in \eqref{eq:T3} produces a
  characteristic geometry for the initial initial-boundary problem
  that is illustrated in Figure \ref{fig:proof-1}. Corners $a$ and $c$
  are obtuse, while $b$ and $d$ are acute. Further, lines $(1,3)$ and
  $(2, 4)$ are each parallel and of the same length, with lines 2 and
  4 being longer. This can be observed from the coordinates of the
  four corners defining $\tilde{\Omega}$ in the transformed topology:
  \begin{align*}
    a &= (0,\quad 0),&
                       b &= \frac{\sqrt{2}}{2} \left( \frac{1}{\sqrt{1-\rho}},\quad \frac{1}{\sqrt{1+\rho}} \right), \\
    c &= \frac{\sqrt{2}}{2} \left( \frac{1-1/\sigma_{\tilde{y}}}{\sqrt{1-\rho}},\quad \frac{1+1/\sigma_{\tilde{y}}}{\sqrt{1+\rho}} \right),&
                                                                                                                                     d &= \frac{\sqrt{2}}{2} \left( \frac{-1/\sigma_{\tilde{y}}}{\sqrt{1-\rho}},\quad \frac{1/\sigma_{\tilde{y}}}{\sqrt{1+\rho}} \right), \\
    IC &= \frac{\sqrt{2}}{2} \left( \frac{x_0 - y_0/\sigma_{\tilde{y}}}{\sqrt{1-\rho}},\quad \frac{x_0 + y_0/\sigma_{\tilde{y}}}{\sqrt{1-\rho}} \right)
  \end{align*}

  Now consider placing the images $j = 1, \ldots, J$ by performing a
  finite number of alternating reflections about lines 4 and 2. This
  places images along a finite segment of a line running through the
  initial condition position and orthogonal to lines 2 and 4 (dashed line and
  black/red dots in Figure \ref{fig:proof-1}). If the number of
  reflections is kept constant and $\sigma_{\tilde{y}}$ is sufficiently
  small, all thus placed images are in the interior region
  formed by extending lines 1 and 3. This is proved from the following
  observations:
  \begin{enumerate}
  \item The slopes of both lines 2 and 4 are equal to
    $-\frac{\sqrt{1+\rho}}{\sqrt{1-\rho}}$ and are therefore
    \textit{independent of the choice} of $\sigma_{\tilde{y}}$. Hence,
    shrinking $\sigma_{\tilde{y}}$ moves corners $d$ and $c$ along
    lines 4 and 2, respectively, away from the origin and thereby
    increasing the interior region formed by extending lines 1 and 3.
  \item From observation $1$, the length of the chord covering the
    reflection line within $\tilde{\Omega}$ is a constant independent of
    $\sigma_{\tilde{y}}$.  This implies that the length of the finite
    segment covering the reflected images is also constant and
    independent of $\sigma_{\tilde{y}}$.
  \item The coordinate of the point of intersection between the
    reflection line and the extension of line 3 (blue point in Figure
    \ref{fig:proof-1}) is $(x,y)$ where
    \begin{align*}
      x &= \frac{\sqrt{2}}{2} \left( \frac{-2 x_0\rho + 2
      y_0/\sigma_{\tilde{y}} -
      2/\sigma_{\tilde{y}}(1-\rho)}{-2\rho\sqrt{1-\rho}}\right), \\
      y &= \frac{\sqrt{2}}{2} \left( \frac{\sqrt{1-\rho}}{\sqrt{1+\rho}}\frac{-2 x_0\rho + 2
      y_0/\sigma_{\tilde{y}} -
      2/\sigma_{\tilde{y}}(1-\rho)}{-2\rho\sqrt{1-\rho}} +
      \frac{1}{\sqrt{2}\sigma_{\tilde{y}}\sqrt{1+\rho}} \right).
    \end{align*}
    The lengths of the $x-$ and $y-$segments of the chord connecting
    the initial condition and the point of intersection are
    \begin{align*}
      \Delta x &= \frac{\sqrt{2}}{2}\left\{ \frac{1}{\sigma_{\tilde{y}}}\cdot\frac{\sigma_{\tilde{y}}x_0 - y_0/\rho + (1-\rho)/\rho + y_0}{\sqrt{1-\rho}} - \frac{x_0}{\sqrt{1-\rho}} \right\},\\
      \Delta y &= \frac{\sqrt{2}}{2} \left\{ \frac{1}{\sigma_{\tilde{y}}}\cdot\frac{(\sigma_{\tilde{y}}x_0 - 2y_0/\rho + (1-\rho)/\rho + 1/\sqrt{2})\sqrt{1-\rho} - y_0\sqrt{1+\rho}}{\sqrt{1-\rho}\sqrt{1+\rho}}- \frac{x_0}{\sqrt{1-\rho}} \right\}.
    \end{align*}
    Hence, as $\sigma_{\tilde{y}} \to 0$, both $\Delta x^2 \to \infty$ and
    $\Delta y^2 \to \infty$, so that the distance between the point of
    intersection and the initial condition grows. The symmetry of the problem (in
    terms of reflections $x \to -x$ and $y \to -y$) makes the argument
    applied to line 3 and the line of reflection applicable to the
    extension of line 1 as well. Figures \ref{fig:proof-2} and
    \ref{fig:proof-3} show the reflected images falling within the
    interior region with $\sigma_{\tilde{y}} = 0.1$ in both geometries.
  \end{enumerate}
  Observations ii) and iii) imply that we can always find a
  sufficiently small $\sigma_{\tilde{y}}$ to cover the line segment
  containing the positions of images resulting from a finite number of
  reflections about boundaries 2 and 4.  Hence, any subsequent
  reflections about lines 1 and 3 are guaranteed to place reflected
  images outside of the computational region $\Omega$.

  For any such finite, admissible collection of images, we can make
  $t^*$ sufficiently small such that the boundary values are
  numerically enforced on three out of the four boundaries (since with
  a finite set of reflections the boundaries are analytically enforced at the
  last boundary of reflection). Therefore, we have a collection of
  images $J$ whose sum satisfies the initial-boundary problem
  \textit{and} is non-trivially differentiable with respect to the four
  boundary parameters for $\tilde{\Omega}$. Moreover, the reflections
  $J$ cover the required set in \eqref{eq:union}.
\end{proof}

%%
\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{../chapter-3/chapter-3-figure-proof-3.pdf}
  \caption{Set of images in the original topology when
    $\sigma_{\tilde{y}} = 0.1$. All images fall outside of
    $\tilde{\Omega}$ for $\rho = 0.9$.}
  \label{fig:proof-3}
\end{figure}


\begin{figure}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/chapter-3-figure-proof-2.pdf}
      \caption{Set of images in the transformed topology when
        $\tilde{\sigma} = 0.1$. All images fall outside of $\tilde{\Omega}$
        for $\rho = 0.9$.}
      \label{fig:proof-2}
\end{figure}

\section{Matching the Small-time and Galerkin Solutions}
So far we have developed an analytic likelihood solution valid for
certain small $\tilde{t}_\epsilon$ determined by the geometry of the
initial-boundary value problem. The Galerkin likelihood solution from
Chapter \ref{ch:galerkin}, on the other hand, applies for relatively
larger $\tilde{t}$.  The results of the simulation studies in Chapter
\ref{ch:galerkin} showed the need to resolve the likelihood in the
\textit{transient} region $\tilde{t}$ between where the small-time
likelihood is applicable and where the Galerkin likelihood is
applicable. This is achieved by a two-step process where we first
approximate the Galerkin likelihood with a few low-frequency modes via
least squares then match the first two derivatives of the small-time
likelihood to those of the low-frequency approximation. The resultant
interpolation is a matching solution which also resolves the transient
region.

The likelihood computed with the Galerkin solution as a
function of $\tilde{t}$ is of the form
\begin{align*}
  \frac{\partial^4 p_{G}(\tilde{x}, \tilde{y}, \tilde{t})}{\partial a_x
  \partial b_x \partial a_y \partial b_y} = \sum_{k=1}^{K} e^{-\lambda_k\tilde{t}} p_k^{(4)}(\tilde{t}),
\end{align*}
where $p_i^{(4)}(\tilde{t})$ is a fourth-order polynomial. This
proceeds from the Galerkin solution being dependent on $\tilde{t}$
only through the exponential term: the eigenfunctions of the solution
are by design solely functions of $(a_x, b_x, a_y, b_y)$ and
$(\tilde{x}, \tilde{y})$. Expanding the polynomials, we see that the
generating functions for the likelihood are of the form
\begin{align}
  g_{k,j}(t) &:= e^{-\lambda_k \tilde{t}}\, t^{\alpha_j}, & \alpha_j = 0, 1, 3, 4.
\end{align}
The Laplace approximation of $g_{k,j}$ with $\alpha_j > 0$ is useful
in elucidating how each of the generating functions contribute to the
overall likelihood. Letting
\begin{align}
  \tilde{T} = \log(\tilde{t}), 
\end{align}
to stabilize the tails of the approximant and performing a
second-order Taylor expansion of $g(\tilde{T})$ about the maximum
of $g_{k,j}$ produces the Gaussian approximation
\begin{align}
  g_{k,j}(\tilde{T}) \approx C(\lambda_k, \alpha_j) \exp\left\{ -\frac{1}{2}\alpha_j (\tilde{T} - \log(\alpha_j/\lambda_k))^2 \right\}. \label{eq:G-approx}
\end{align}
Up to second order, therefore, the Galerkin likelihood (as a function
of $\tilde{t}$) is a linear combination of Gaussian kernels $g_{k,j}$
whose bandwidth is controlled by the degree of polynomial multiplier
$\alpha_j$ and whose location is controlled by the eigenvalue
$\lambda_j$ and $\alpha_j$. In particular, the solution exhibits the
behavior of resolving smaller $\tilde{t}$ regions with
higher-frequency modes. The Laplace approximation of the generating
functions is especially appropriate for the transient region of
$\tilde{t}$. There, the ``non-Gaussian'' generating functions
$e^{-\lambda_k\tilde{t}}$ tend to unity, while the approximately
Gaussian $g_{k,j}$ are dominated by the $\tilde{t}^{\alpha_j}$ terms
(left-hand tails of \eqref{eq:G-approx}).

The first part of the matched solution is constructed from the
generating functions $g_{k,j}$ by fitting via least squares to values
produced outside the transient region, $i.e.$ using big-$\tilde{t}$
likelihood values. We need to use as few points as possible in the
fitting exercise, as evaluating the Galerkin likelihood is relatively
expensive, while still accurately representing the right-hand tail of
the solution in the big-$\tilde{t}$ region. For this reason, we choose
a sparse representation of the likelihood using the first two
generating functions $g_{1,4}, g_{2,4}$ as proportional to the matched
solution
\begin{align*}
  f_{LS}(\tilde{t}) &= \tilde{t}^4 \left( \omega_1 e^{-\lambda_1\tilde{t}} + \omega_2 e^{-\lambda_2\tilde{t}}\right), \\
  \log f_{LS}(\tilde{t})&= 4\log(\tilde{t}) - \lambda_1\tilde{t} + \log\left(\omega_1 + \omega_2e^{-(\lambda_2-\lambda_1)\tilde{t}} \right)
\end{align*}
where $f_{LS}$ is asymptotically valid for large $\tilde{t}$. Also,
the largest possible degree of $\tilde{t}^{\alpha}$ used can more
easily fit the small-$\tilde{t}$ likelihood which rapidly drops near
$\tilde{t} = 0$. The weights $\omega_{1}, \omega_{2}$ are estimated
from two likelihood values outside the transient region. The two
points of evaluation are $\tilde{t}_1 = 0.80$ and
$\tilde{t}_2 = 1.80$, chosen based on the empirical behavior of the
Galerkin solver observed in Figures \ref{fig:limitations-rho-0.95-1} -
\ref{fig:limitations-rho-0.95-3}. If the Galerkin solver produces an
invalid likelihood at $\tilde{t} = 0.80$ or $\tilde{t} = 1.80$, then
both trial points are increased by $1$ until valid likelihoods are
produced. The reason for the large jumps in this case is the
relatively high cost of using the Galerkin solver and the guarantee of
the existence of a valid likelihood for sufficiently large
$\tilde{t}$. Once obtained, the two likelihood values are used to
compute the best-fitting $\omega_{1}, \omega_{2}$ via least
squares. This matching scheme is applied to the same points as in
Figures \ref{fig:limitations-rho-0.95-1} -
\ref{fig:limitations-rho-0.95-3} and are shown in Figures
\ref{fig:matched-rho-0.95-1} - \ref{fig:matched-rho-0.95-3}. This
least-squares (LS) solution is used to approximate the location
$\tilde{t}_m$ and value of the maximum of the likelihood, as well as
compute its first two derivatives there analytically. This data is the
right-hand side of the matching solution. We pick this point, because
it is close to the maximum under the Galerkin solution and as such it
is the inflection point where the dynamics of the solution move from
being dominated by the $-\beta/\tilde{t}$ term (as we show below) to
those dominated by $-\lambda_1\tilde{t}$ term. Next, we use the
small-time likelihood to pick the functional form of the matching
solution that interpolates between the small-time region and the
maximum point of the Galerkin solution, as well as derive the data
necessary for the left-hand side of the matching.

Closer observation of (\ref{eq:truncated-approx}) reveal that each of
the summands of the truncated small-time solution share a common
polynomial factor:
\begin{align*}
  \frac{\partial^4 p_\epsilon(\tilde{x}, \tilde{y}, \tilde{t})}{\partial a_x
  \partial b_x \partial a_y \partial b_y} &\approx \\
  \MoveEqLeft[10] \approx \sum_{j' = 1}^{4} \frac{1}{\pi\sqrt{2}} \left(\frac{1}{2\,\,\tilde{t}\, \sigma_{\tilde{y}}^2 (1-\rho^2)} \right)^{4.5}
  \exp\left( -\frac{1}{2\,\,\tilde{t}\, \sigma_{\tilde{y}}^2 (1-\rho^2)} \mathcal{P}_{j'}\right)
  \left(\partial_{a_x}\partial_{b_x} \partial_{a_y}\partial_{b_y} \right)\mathcal{P}_{j'} \\
                                          &= K \left(\frac{1}{\tilde{t}} \right)^{4.5} \sum_{j'=1}^4 c_j' \exp\left( -\frac{\beta_{j'}}{\tilde{t}} \right), \\
  K &= \frac{1}{\pi\sqrt{2}} \left(\frac{1}{2\,\, \sigma_{\tilde{y}}^2 (1-\rho^2)} \right)^{4.5}, \\
  \beta_{j'} &= \frac{1}{2\,\, \sigma_{\tilde{y}}^2 (1-\rho^2)} \mathcal{P}_{j'}, \\
  c_{j'} &= \left(\partial_{a_x}\partial_{b_x} \partial_{a_y}\partial_{b_y} \right)\mathcal{P}_{j'}
\end{align*}
The summand with the greatest $\beta_{j'}$ contributes the most to the
truncated small-time solution in the $\tilde{t} \leq 1$ region where
the matched solution will be applied. Indexing $j'$ such that
$\beta_1 \geq \beta_2 \geq \beta_3 \geq \beta_4$, the small-time
log-likelihood is
\begin{align}
  \log\left( \frac{\partial^4 p_\epsilon(\tilde{x}, \tilde{y}, \tilde{t})}{\partial a_x
  \partial b_x \partial a_y \partial b_y} \right) &\approx \log(K) - 4.5\log(\tilde{t}) + \log(c_1) - \frac{\beta_1}{\tilde{t}} \nonumber \\
  &\quad + \log\left(1 + \sum_{j \neq 1} \frac{c_j}{c_1}\exp\left( -\frac{(\beta_j-\beta_1)}{\tilde{t}} \right) \right) \nonumber \\
  &\approx \log(K) - 4.5\log(\tilde{t}) + \log(c_1) - \frac{\beta_1}{\tilde{t}} + \log\left(1 + \epsilon(\tilde{t}) \right), \label{eq:small-time-log-like}
\end{align}
where we are assuming that the non-dominant exponential terms are
small relative to the $\beta_1$ term. When
constructing the matched solution, we ignore $\epsilon(\tilde{t})$
since the likelihood function is dominated by the $\log(\tilde{t})$
term away from zero.

The assumed form for the matched solution is the same as
(\ref{eq:small-time-log-like}):
\[
  \log f_{\mbox{matched}}(\tilde{t}) = \log(\omega(\tilde{t})) -
  \gamma(\tilde{t})\log(\tilde{t}) -
  \frac{\beta(\tilde{t})}{\tilde{t}}
\]
where the parameters $\omega(\tilde{t}), \gamma(\tilde{t}),$ and
$\beta(\tilde{t})$ vary with $\tilde{t}$. At $\tilde{t}^*$, the
left-hand side of the matching condition, the values for these
parameters are defined such that they match the small-time solution
\begin{align*}
  \omega(\tilde{t}^*) &= K, & \gamma(\tilde{t}^*) &= 4.5, & \beta(\tilde{t}^*) &= \beta_1.
\end{align*}
At $\tilde{t}_m$, the right-hand side of the matching condition and
the maximum of the LS solution,
$\omega(\tilde{t}), \gamma(\tilde{t}),$ and $\beta(\tilde{t})$ are
chosen to match the value, first, and second derivatives of the
logarithmic form of the LS solution. The form of the parameters
between $\tilde{t}^*$ and $\tilde{t}_m$ is chosen to be a sigmoid
function which rapidly transitions away from $\tilde{t}^*$ and is the
same for all three parameters:
\begin{align}
  \begin{split}
    \omega(\tilde{t}) &= \omega(\tilde{t}^*)e^{-k(\tilde{t}-\tilde{t}^*)} + \omega(\tilde{t}_m)\left(1-e^{-k(\tilde{t}-\tilde{t}^*)}\right), \\
    \gamma(\tilde{t}) &= \gamma(\tilde{t}^*)e^{-k(\tilde{t}-\tilde{t}^*)} + \gamma(\tilde{t}_m)\left(1-e^{-k(\tilde{t}-\tilde{t}^*)}\right), \\
    \beta(\tilde{t}) &= \beta(\tilde{t}^*)e^{-k(\tilde{t}-\tilde{t}^*)} + \beta(\tilde{t}_m)\left(1-e^{-k(\tilde{t}-\tilde{t}^*)}\right).
  \end{split}
\end{align}
Given the order magnitudes of $\tilde{t}^*$ and $\tilde{t}_m$ commonly
encountered, $k \approx 100$ is sufficiently large such that the
higher-order derivatives of $1-e^{-k(\tilde{t}-\tilde{t}^*)}$ are
approximately zero at $\tilde{t}_m$ and the matching on the right-hand
side is still satisfied.

\section{MLEs, revisited}

\begin{figure}
  \centering
  %%
  %%
  \begin{tabular}{ccc}
    \begin{minipage}{0.3\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/results/mle-results-rho-0.95-n-16/estimates-rho.pdf}
    \end{minipage}
    & \begin{minipage}{0.3\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/results/mle-results-rho-0.95-n-16/estimates-sigma-x.pdf}
    \end{minipage}
    & \begin{minipage}{0.3\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/results/mle-results-rho-0.95-n-16/estimates-sigma-y.pdf}
    \end{minipage} \\
    \begin{minipage}{0.3\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/results/mle-results-rho-0.95-n-8/estimates-rho.pdf}
    \end{minipage}
    & \begin{minipage}{0.3\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/results/mle-results-rho-0.95-n-8/estimates-sigma-x.pdf}
    \end{minipage}
    & \begin{minipage}{0.3\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/results/mle-results-rho-0.95-n-8/estimates-sigma-y.pdf}
    \end{minipage} \\
    \begin{minipage}{0.3\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/results/mle-results-rho-0.95-n-4/estimates-rho.pdf}
    \end{minipage}
    & \begin{minipage}{0.3\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/results/mle-results-rho-0.95-n-4/estimates-sigma-x.pdf}
    \end{minipage}
    & \begin{minipage}{0.3\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/results/mle-results-rho-0.95-n-4/estimates-sigma-y.pdf}
    \end{minipage}
  \end{tabular}
  \caption{Data generated with $\rho=0.95$ and is the same as in
    Figure \ref{fig:mle-comparison-rho-0.95}. Kernel-density
    approximations of the repeated-sampling densities of the MLEs are
    shown.  Samples are obtained from the Galerkin likelihood with the
    matching solution (green) and the classical likelihood (red) and
    the Rogers estimator (blue). The data-generating parameters are
    denoted with the vertical solid line. Compared to Figure
    \ref{fig:mle-comparison-rho-0.95}, the repeated sampling
    distributions of the parameter estimates based on the
    Galerkin/matching solution are tighter.}
  \label{fig:mle-comparison-rho-0.95-matched}
\end{figure}

To complete the analysis, we consider again the simulation scenario in
Chapter \ref{ch:galerkin} and the experimental setup where the
Galerkin solution alone failed to outperform the Gaussian
(non-boundary) likelihood. In particular, $\rho=0.95$ with $m = 8,$
and $16$ showed a greater MSE than the MLE estimates using a purely
Gaussian likelihood (see Table \ref{tab:MSE-ratios}). Using the same
simulated data sets, the repeated sampling distribution approximations
where the matching solution is used in addition to the Galerkin
likelihood are shown in Figure
\ref{fig:mle-comparison-rho-0.95-matched}. The ratios of the MSEs
between the matched and Gaussian likelihoods are shown in Table
\ref{tab:MSE-ratios-matched}. We can see that the MSEs are
resoundingly better, and that using the matching technique developed
here improves the statistical power of using OCHL data.

\begin{table}
  \centering
  \begin{tabular}{cccc}
    \multicolumn{4}{c}{$\rho=0.95$} \\
    & $m=4$ & $m=8$ & $m=16$ \\
    \hline
    $\hat{\sigma}_x$ & 0.124 & 0.429  & 0.318 \\
    \hline
    $\hat{\sigma}_y$ & 0.310 & 0.147  & 0.365 \\
    \hline
    $\hat{\rho}$ & 0.250 & 0.753 & 0.699
  \end{tabular}
  \caption{Ratios of Galerkin to Gaussian MSEs for the three simulation cases.}
  \label{tab:MSE-ratios-matched}
\end{table}


% The log-likelihood in this case is
% \begin{multline}
%   \log\left(\frac{\partial^4 p_{\mbox{Galerkin}}(\tilde{x}, \tilde{y}, \tilde{t})}{\partial a_x \partial b_x \partial a_y \partial b_y}\right) = -\lambda_1\tilde{t} + \log(p_1^{(4)}(\tilde{t})) + \log\left( 1 + \sum_{k=2}^K e^{-(\lambda_k-\lambda_1)\tilde{t}}\frac{p_k^{(4)}(\tilde{t})}{p_1^{(4)}(\tilde{t})} \right) \label{eq:galerkin-log-like}
% \end{multline}
% % As in the small-time likelihood, we ignore the effects of higher order
% % eigenvalues and is appropriate to do so as long as
% % $e^{-(\lambda_k-\lambda_1)\tilde{t}}$ is small relative to 1.

% To construct the matched solution, we first approximate
% \eqref{eq:galerkin-log-like} with the ansatz
% \begin{align}
%     \log\left(\frac{\partial^4 p_{\mbox{Galerkin}}(\tilde{x}, \tilde{y}, \tilde{t})}{\partial a_x
%       \partial b_x \partial a_y \partial b_y}\right) \approx C + \alpha\log(\tilde{t}) - \lambda_1\tilde{t}. \label{eq:ansatz}
% \end{align}
% The parameters $\alpha$ and $C$ are found by evaulating the likelihood
% at two points $\tilde{t}_1$ and $\tilde{t}_2$ in the computable
% domain. Figures \ref{fig:limitations-rho-0.95-1} -
% \ref{fig:limitations-rho-0.95-3} suggest that suitable candidates for
% these points are $\tilde{t}_1 = 0.5$ and $\tilde{t}_2 = 2$ since they
% mostly cover the inflection point of the likelihood function but stay
% within the computable region of the parameter space. Next, we posit
% that the log-likelihood in the transient region has the form
% \begin{align}
%   f_{\mbox{transient}}(\tilde{t}) = \kappa - \gamma\log(\tilde{t}) - \frac{\beta_1}{\tilde{t}}. \label{eq:transient}
% \end{align}
% This is the same form as \eqref{eq:small-time-log-like}, where we have
% introduced two degrees of freedom in the parameters $\kappa$ and
% $\gamma$ to match the Galerkin log-likelihood ansatz up to first
% order. We find $\gamma$ by setting it such that the maximum of
% \eqref{eq:transient} occurs at the same value $\tilde{t}^*$ as for
% \eqref{eq:ansatz}. Then $\kappa$ is found such that the transient form
% \eqref{eq:transient} values matches that of the Galerkin solution at
% $\tilde{t}^*$. Figure \ref{fig:matched-rho-0.95-1} details the method
% for a single data point and demonstrates a typical outcome when using
% this technique. Figures \ref{fig:matched-rho-0.95-2} and
% \ref{fig:matched-rho-0.95-3} demonstrate the technique applied to the
% data points introduced in Chapter \ref{ch:galerkin} to ascertain the
% applicable range for the Galerkin likelihood. 

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{../chapter-3/figures/{matched-rho-0.95-data-point-4}.pdf}
  \caption{Galerkin and matched log-likelihoods for the point in
    Figure \ref{fig:limitations-rho-0.95-1}, computed with different
    $(\tilde{\rho} = 0.60, \tilde{\sigma} = 0.08)$. Also shown are the
    least-squares solution and the matched solution. We see that the
    location of the maximum of the least-squares solution is very close
    to the maximum of the Galerkin solution. The matched solution
    preserves the asymptotic behavior of the small-time likelihood but
    also matches the Galerkin likelihood at the peak.}
  \label{fig:matched-rho-0.95-1}
\end{figure}

\begin{figure}
  \centering
  \begin{tabular}{cc}
    \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/figures/{matched-rho-0.95-data-point-2}.pdf}
    \end{minipage}
    & \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/figures/{matched-rho-0.95-data-point-3}.pdf}
    \end{minipage} \\
        \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/figures/{matched-rho-0.95-data-point-1}.pdf}
    \end{minipage}
    & \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/figures/{matched-rho-0.95-data-point-5}.pdf}
    \end{minipage} \\
    \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/figures/{matched-rho-0.95-data-point-6}.pdf}
    \end{minipage}
    & \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/figures/{matched-rho-0.95-data-point-7}.pdf}
    \end{minipage}
  \end{tabular}
  \caption{Data points 2 through 12 used in Figure
    \ref{fig:limitations-rho-0.95-2} with the Galerkin solution and
    the matched asymptotic solution in the transient region. The
    parameters for the Galerkin solution are
    $(\tilde{\rho} = 0.60, \tilde{\sigma} = 0.08, l=1)$}
  \label{fig:matched-rho-0.95-2}
\end{figure}


\begin{figure}
  \centering
  \begin{tabular}{cc}
    \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/figures/{matched-rho-0.95-data-point-8}.pdf}
    \end{minipage}
    & \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/figures/{matched-rho-0.95-data-point-9}.pdf}
    \end{minipage} \\
        \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/figures/{matched-rho-0.95-data-point-10}.pdf}
    \end{minipage}
    & \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/figures/{matched-rho-0.95-data-point-11}.pdf}
    \end{minipage} \\
    \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/figures/{matched-rho-0.95-data-point-12}.pdf}
    \end{minipage}
    & \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=1\linewidth]{../chapter-3/figures/{matched-rho-0.95-data-point-13}.pdf}
    \end{minipage}
  \end{tabular}
  \caption{Data points 8 through 13 used in Figure
    \ref{fig:limitations-rho-0.95-3} with the Galerkin solution and
    the matched asymptotic solution in the transient region. The
    parameters for the Galerkin solution are
    $(\tilde{\rho} = 0.60, \tilde{\sigma} = 0.08, l=1)$}
  \label{fig:matched-rho-0.95-3}
\end{figure}
