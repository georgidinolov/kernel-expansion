\documentclass[10pt]{article}

\usepackage{amsthm}
\usepackage[toc,page]{appendix}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{pslatex,palatino,avant,graphicx,color}
\usepackage{colortbl}
\usepackage{fullpage}
\usepackage{mathtools}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{caption}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{enumerate}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{bbm}
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents

\newtheorem{lemma}{Lemma}
%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{0in}%
\addtolength{\evensidemargin}{0in}%
\addtolength{\textwidth}{0in}%
\addtolength{\textheight}{0in}%
\addtolength{\topmargin}{0in}%


\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

\newcommand{\indicator}[1]{\mathbbm{1}\left( #1 \right) }
\newcommand{\hb}{\hat{b}}
\newcommand{\ha}{\hat{a}}
\newcommand{\htheta}{\hat{\theta}}
\newcommand{\halpha}{\hat{\alpha}}
\newcommand{\hmu}{\hat{\mu}}
\newcommand{\hsigma}{\hat{\sigma}}
\newcommand{\hphi}{\hat{\phi}}
\newcommand{\htau}{\hat{\tau}}
\newcommand{\heta}{\hat{\eta}}
\newcommand{\E}[1]{\mbox{E}\left[#1\right]}
\newcommand{\Var}[1]{\mbox{Var}\left[#1\right]}
\newcommand{\Indicator}[1]{\mathbbm{1}_{ \left( #1 \right) } }
\newcommand{\dNormal}[3]{ N\left( #1 \left| #2, #3 \right. \right) }
\newcommand{\Beta}[2]{\mbox{Beta}\left( #1, #2 \right)}
\newcommand{\alphaphi}{\alpha_{\hphi}}
\newcommand{\betaphi}{\beta_{\hphi}}
\newcommand{\expo}[1]{ \exp\left\{ #1 \right\}}
\newcommand{\tauSquareDelta}{\htau^2
  \left(\frac{1-\expo{-2\htheta\Delta}}{2\htheta} \right)}
\newcommand{\mumu}{\mu_{\hmu}}
\newcommand{\sigmamu}{\sigma^2_{\hmu}}
\newcommand{\sigmamuexpr}{\log\left( \frac{\VarX}{\EX^2} + 1 \right)}
\newcommand{\mumuexpr}{\log(\EX) -  \log\left( \frac{\VarX}{\EX^2} + 1 \right) /2 }

\newcommand{\EX}{\mbox{E}\left[ X \right] }
\newcommand{\VarX}{\mbox{Var}\left[ X \right] }
\newcommand{\mueta}{\mu_{\heta} }
\newcommand{\sigmaeta}{\sigma^2_{\heta}}
\newcommand{\sigmaetaexpr}{ \log\left( \frac{\VarX}{\EX^2} + 1 \right) }
\newcommand{\muetaexpr}{ \log(\EX) -  \sigmaetaexpr /2 }

\newcommand{\mualpha}{\mu_{\halpha} }
\newcommand{\sigmaalpha}{\sigma^2_{\halpha}}
\newcommand{\sigmaalphaexpr}{ \log\left( \frac{\VarX}{\EX^2} + 1 \right) }
\newcommand{\mualphaexpr}{ \log(\EX) -  \sigmaalphaexpr /2 }

\newcommand{\mutauexpr}{ \frac{2}{T} \EX }
\newcommand{\sigmatauexpr}{ \frac{4}{T^2} \Var{X}}

\newcommand{\alphatau}{\alpha_{\htau^2}}
\newcommand{\betatau}{\beta_{\htau^2}}

\newcommand{\Gam}[2]{\mbox{Gamma}\left( #1, #2 \right) }
\newcommand{\InvGam}[2]{\mbox{Inv-Gamma}\left( #1, #2 \right) }

%%% END Article customizations

%%% The "real" document content comes below...

% \newbox{\LegendeA}
% \savebox{\LegendeA}{
%    (\begin{pspicture}(0,0)(0.6,0)
%    \psline[linewidth=0.04,linecolor=red](0,0.1)(0.6,0.1)
%    \end{pspicture})}
% \newbox{\LegendeB}
%    \savebox{\LegendeB}{
%    (\begin{pspicture}(0,0)(0.6,0)
%    \psline[linestyle=dashed,dash=1pt 2pt,linewidth=0.04,linecolor=blue](0,0.1)(0.6,0.1)
%    \end{pspicture})}

\title{A Range-Based Bivariate Stochastic Volatility Model}
\author{Georgi Dinolov, Abel Rodriguez, Hongyun Wang}
\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed

\begin{document}
\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

\bigskip

\vspace{1cm}
\noindent

\spacingset{1.00} %
\section{Introduction}

The estimation and prediction of price volatility from market data is
an important problem in econometrics and finance
\citep{abramov2007estimation}, as well as practical risk management
\citep{brandt2006dynamic}. The literature on the subject of volatility
estimation is vast. Model-based approaches for a single observable
asset begin with the ARCH and GARCH models of \cite{engle1982} and
\cite{bollerslev1986}, moving on to stochastic volatility models (see
\cite{shephard2005selected-readings}, for example).

Multivariate equivalents for each of these model classes exist (see
\cite{bauwens2006multivariate} and \cite{asai2006multivariate} for
reviews of multivariate GARCH and for multivariate stochastic
volatility, respectively). However, the majority of work on the
subject uses opening and closing prices as data. This approach
invariably disregards information traditionally contained in financial
timeseries: the observed high and low price of an asset over the
quoted periods. To our current knowledge, only \cite{rodriguez2012}
use the observed maximum and minimum of prices in a likelihood to
estimate volatility. They do so, however, in a univariate
setting.

Explicit model-based approaches in the multivariate setting which take
into account extrema over observational periods are completely lacking
in the literature, because deriving an efficient approximation of the
corresponding likelihood function has hereto been an open problem. In
this paper, we use a result addressing this problem and introduce a
\textit{bivariate} stochastic volatility model which takes into
account the highest and lowest observed prices of each asset as part
of a likelihood-based (Bayesian) estimation procedure.

\section{Model}
The model we will estimate is a bivaraite, 1-factor stochastic volatility
model with leverage:

\begin{align}
  \left( \begin{array}{c}
           x_t \\
           y_t
         \end{array} \right) &= \left( \begin{array}{c}
                                         x_{t-\Delta} \\
                                         y_{t-\Delta}
                                       \end{array} \right) +
  \left( \begin{array}{c}
           \mu_x\Delta \\
           \mu_y\Delta \end{array} \right) +
  \left( \begin{array}{cc}
           \sqrt{1-\rho_t^2}\sigma_{x,t} & \rho_t \sigma_{x,t} \\
           0 & \sigma_{y,t}
         \end{array} \right)
               \left( \begin{array}{c}
                        \epsilon_{x,t} \\
                        \epsilon_{y,t}
                      \end{array} \right), \label{eq:process-evolution}\\
  \inf_{[t-\Delta,t]} x_\tau &= a_{x,t}&  \sup_{[t-\Delta,t]} x_\tau &= b_{x,t} & \inf_{[t-\Delta,t]} y_\tau &= a_{y,t}
                                                                     & \sup_{[t-\Delta,t]} y_\tau &= b_{y,t} \nonumber \\
  \log(\sigma_{x,t+\Delta}) &= \alpha_x + \theta_x(\log(\sigma_{x,t}) - \alpha_x) + \tau_x \eta_{x,t}, \\
  \log(\sigma_{y,t+\Delta}) &= \alpha_y + \theta_y(\log(\sigma_{y,t}) - \alpha_y) + \tau_y \eta_{y,t}, \\
  \mbox{logit}((\rho_{t+\Delta} + 1)/2) &= \alpha_\rho + \theta_\rho\left(\mbox{logit}((\rho_{t}+1)/2) - \alpha_\rho\right) + \tau_{\rho} \eta_{\rho,t}. \label{eq:correlation-evolution}
\end{align}
The marginal distribution for all of the innovation terms
$\epsilon_{x,t}, \epsilon_{y,t}, \eta_{x,t}, \eta_{y,t},
\eta_{\rho,t}$ is the standard Gaussian distribution. The
\textit{leverage} terms are defined as
$E\left[\epsilon_{x,t}\, \eta_{x,t}\right] = \rho_{x}$ and
$E\left[\epsilon_{x,t}\, \eta_{x,t}\right] = \rho_{y}$. It should be
noted here that we are explicitly allowing the correlation of the
process to change over time in a mean-reverting fashion. Finally, we
explicitly write down the realized extrema over the periods
$[t-\Delta,t]$ to be included as data into the likelihood for the
dynamical model. We estimate all parameters and dynamical factors in a
fully Bayes framework via the augmented particle filter of
\cite{liu2001combined} which we will describe below.

\subsection{Likelihood for the observables}
Each period $[t-\Delta,t]$ has six associated observables: opening
coordinate $(x_{t-\Delta},y_{t-\Delta})$, closing coordinate
$(x_{t},y_{t})$, and the observed extrema in each nominal direction
$(a_{x,t}, b_{x,t}), (a_{y,t},b_{y,t})$. Given the evolution model in
(\ref{eq:process-evolution}), disregarding the information contained in
the extrema yields the usual bivariate Gaussian density in terms of
the volatility parameters and the state of the process at time $t-\Delta$:

\[
  p(x_t,y_t| x_{t-\Delta}, y_{t-\Delta}, \mu_x, \mu_y, \sigma_{x,t}, \sigma_{y,t}, \rho_t) =
\]
\[
  \frac{1}{2\pi\,\,\Delta\,\,
    \sigma_{x,t}\sigma_{y,t}\sqrt{1-\rho_t^2}} \exp\left\{
    -\frac{1}{2\,\Delta(1-\rho_t^2)} \left( \frac{(x_t -
        x_{t-\Delta})^2}{\sigma_{x,t}^2} - 2\rho_t
      \frac{(x_{t}-x_{t-\Delta})(y_t-y_{t-\Delta})}{\sigma_{x,t}\sigma_{y,t}}
      + \frac{(y_t - y_{t-\Delta})^2}{\sigma_{y,t}^2}\right) \right\}.
\]
Incorporating the extreme values over $[t-\Delta,t]$ is accomplished
by considering the Fokker-Planck Equation for the forward,
continuous-time evolution of the probability density function of
$(x_t, y_t)$ and including $(a_{x,t}, b_{x,t}), (a_{y,t},b_{y,t})$ as
boundary conditions where the density is zero. The previous work
describes the method by which the full likelihood function is
found. However, for the purposes of the particle filter used to
estimate the model in this work, we improve upon the computational
method by performing a set of normalizing transformations, allowing
for more flexibility in the parameters for the basis functions in the
Galerkin approximation, and extrapolating over certain low-probability
regions of the parameter space.

\subsection{Improved likelihood computational method for low probability data}
The method we use to estimate the model
(\ref{eq:process-evolution}) - (\ref{eq:correlation-evolution}) is
essentially comprised of a sequence of importance samples whose weight
in the posterior is a function of the ratio of the likelihood function
evaluated at different parameter values. For this reason, our method
for approximating the likelihood needs to be able to accomodate
proposed parameters for given data values in low-likelihood
regions. To ensure this property, the previous method used to
approximate the likelihood is modified in the following way:
\begin{enumerate}
\item When computing the numerical derivative of the Galerkin
  solution, perturb the computational boundaries on an already
  normalized domain. This \textit{does not} require an additional
  transformation if the numerical precision of a double is enough:
  \begin{align}
    \Delta x &= h \cdot L_x, \\
    \Delta y &= h \cdot L_y,
  \end{align}
  where $h$ fixed
  \[
    (L_x + \Delta x) / L_x = 1 + h\cdot L_x / L_x.
  \]

\item Perform either a $\pi/2$ or -$\pi/2$ rotation so that the
  greater normalized diffusion parameter corresponds to the same
  principal direction. Then perform a scaling of the diffusion
  timescale so that the greater diffusion parameter is always unity.

\item Given the constant diffusion parameter in a single direction, we
  can use different resolution with the basis functions in each
  principal direction. In this way we can control separately the slow
  and fast diffusion phenomena for the process.

\item In the cases where the likelihood is invalid (negative), perform
  an extrapolation from a parameter combination that does give a valid
  likelihood approximation.
\end{enumerate}


\section{Particle Filtering}
Bayesian approaches for estimating complex hierarchical models like
those in (\ref{eq:process-evolution}) -
(\ref{eq:correlation-evolution}) are common. The primary focus of such
inferential algorithms is the generation of dependent samples from the
posterior distributions of model parameters and unobservable latent
trajectories via Markov Chain Monte Carlo (MCMC). However, the highly
non-linear nature of the state-space model considered here prohibits
conditional Gaussian representations and thereby limits the
application of MCMC methods. Instead, we use a particle filtering
approach, which is a particular version of a sequential Monte Carlo
algorithm (see \cite{doucet2001sequential} for an overview), to
generate samples from the posterior distribution of the collection of
time-dependent parameters, which we abbreviate to
\[
  \sigma_t := (\sigma_{x,t}, \sigma_{y,t}, \rho_t),
\]
as well as all of the time-constant parameters governing the evolution
of the process, which we transform to be on the $(-\infty, \infty)$
scale and denote
\[
  \phi := (\alpha_x, \alpha_y, \alpha_\rho, \theta_x, \theta_y,
  \theta_\rho, \log(\tau_x), \log(\tau_y), \log(\tau_\rho),
  \mbox{logit}(\rho_x), \mbox{logit}(\rho_y)).
\]
Most sequential Monte Carlo algorithms assume the structural
parameters $\phi$ to be known and fixed, which is not the case for the
considered problem. We therefore follow \cite{rodriguez2012} and use a
version of the augmented particle filter of \cite{pitt1999filtering}
developed by \cite{liu2001combined} to sample from the full
posterior.

Particle filters use a discrete mixture to represent the posterior
distribution $p(\sigma_t, \phi | \mathcal{D}_t)$, where
$\mathcal{D}_t$ represents all of the observable information up to
time $t$:
\[
  \mathcal{D}_t = (x_0, y_0, a_{x,\Delta}, b_{x,\Delta}, a_{y,\Delta},
  b_{y,\Delta}, x_\Delta, y_\Delta, \ldots, x_{t-\Delta},
  y_{t-\Delta}, a_{x,t}, b_{x,t}, a_{y,t}, b_{y,t}, x_t, y_t).
\]
In this mixture approximation, the information about the parameters
given the current data is captured in the parameter values and weights
associated with each particle:
\[
  p(\sigma_t, \phi | \mathcal{D}_t) \approx \sum_{k=1}^K \delta(\sigma_t^{(k)} - \sigma_t) \delta(\phi^{(k)} - \phi) w_t^{(k)}
\]
Given this approximation of $p(\sigma_t, \phi | \mathcal{D}_t)$,
additional information at time $t+\Delta$ is incorporated by updating
each particle weight and parameter values via an appropriately chosen
importance sampling distribution and Bayes' Theorem. In the case of
the augmented particle filter of \cite{pitt1999filtering}, which
treats the structural parameters $\phi$ are known and fixed, the
approximate distribution of the state of the system at time $t$ is
\[
  p(\sigma_t | \phi, \mathcal{D}_t) \approx \sum_{k=1}^K
  \delta(\sigma_t^{(k)} - \sigma_t) w_t^{(k)};
\]
the posterior $p(\sigma_{t+\Delta} | \phi, \mathcal{D}_{t+\Delta})$ is
\textit{augmented} with the particle index $k$ and sampled with Bayes' Theorem:
\begin{align}
  p(\sigma_{t+\Delta}, k | \phi, \mathcal{D}_{t+\Delta}) &\propto
                                                           p(x_{t+\Delta}, y_{t+\Delta}, a_{x,t+\Delta}, b_{x, t+\Delta},
                                                           a_{y,t+\Delta}, b_{y, t+\Delta} | \sigma_{t+\Delta}, k, \phi,
                                                           \mathcal{D}_t)p(\sigma_{t+\Delta},k | \phi, \mathcal{D}_t), \\
                                                         &= p(\mathbf{y}_{t+\Delta} | \sigma_{t+\Delta}, \phi, \mathcal{D}_t) p(\sigma_{t+\Delta} | \sigma_{t}^{(k)}, \phi) w^{(k)}_t. \label{eq:augmented-posterior}
\end{align}
\cite{pitt1999filtering} sample the joint posterior with a proposal
distribution which replaces $\sigma_{t+\Delta}$ with a representative
value in the one-step predictive distribution
$p(\sigma_{t+\Delta} | \sigma_t^{(k)}, \phi)$, such as the predictive
mean. In this way $k$ is sampled then $\sigma_{t+\Delta}$ from the
predictive distribution conditional on $k$. The new weight
$w_{t+\Delta}^{(k)}$ is proportional to the ratio of the sampled
$\sigma_{t+\Delta}$ and representative value in the likelihood for the
data.


\cite{liu2001combined} extend the augemented particle filter of
\cite{pitt1999filtering} to allow for the estimation of the constant
structural parameter $\phi$. This is done by introducing an artificial
evolution for the fixed parameter, which is now indexed by $t$, and
defining the transition density
\[
  p(\phi_{t+\Delta} | \mathcal{D}_t) \approx \sum_{k=1}^K
  N(\phi_{t+\Delta} | a\phi_t^{(k)} + (1-a)\bar{\phi}_t, (1-a^2)^2 V_t),
\]
where $a$ is a discount factor between 0 and 1, $\bar{\phi}_t$ is the
average of samples for $\phi$ at $t$ and $V_t$ is the respective
sample covariance at time $t$. The Gaussian perturbation of $\phi_t$
is appropriate, as we have transformed each of the structural
parameters to $\mathbb{R}$, while the shrinkage kernel approximation
preserves the mean and covariance structure of the posterior
distribution from $t$ to $t+\Delta$. This limits the injection of
entropy into the system that would otherwise occur with simpler,
conditionally independent Gaussian perturbations.

The augmented posterior then becomes
\begin{align}
  p(\sigma_{t+\Delta}, \phi_{t+\Delta}, k | \mathcal{D}_{t+\Delta}) \propto p(\mathbf{y}_{t+\Delta} | \sigma_{t+\Delta}, \phi_{t+\Delta}, \mathcal{D}_t) p(\sigma_{t+\Delta} | \phi_{t+\Delta}, \sigma_{t}^{(k)}, \mathcal{D}_t)N(\phi_{t+\Delta} | a\phi_t^{(k)} + (1-a)\bar{\phi}_t, (1-a^2)^2 V_t) w^{(k)}_t. \label{eq:augmented-posterior-liu}
\end{align}
Sampling from the posterior (\ref{eq:augmented-posterior-liu}) is also
done with via a proposal where $(\sigma_{t+\Delta}, \phi_{t+\Delta})$
are replaced with predictive means conditional on $k$:
\begin{align}
  p(\sigma_{t+\Delta}, \phi_{t+\Delta}, k | \mathcal{D}_{t+\Delta}) &\propto p(\mathbf{y}_{t+\Delta} | E[\sigma_{t+\Delta} | \sigma_{t}^{(k)}, \phi_t^{(k)}], a\phi_t^{(k)} + (1-a)\bar{\phi}_t) \nonumber \\
  & \times p(\sigma_{t+\Delta} | a\phi_t^{(k)} + (1-a)\bar{\phi}_t, \sigma_{t}^{(k)}) N(\phi_{t+\Delta} | a\phi_t^{(k)} + (1-a)\bar{\phi}_t, (1-a^2)^2 V_t) w^{(k)}_t. \label{eq:proposal-liu}
\end{align}
With (\ref{eq:proposal-liu}), we can integrate out
$(\sigma_{t+\Delta}, \phi_{t+\Delta})$ to propose $k$, then sample the
two remaining parameters respectively. The new weights are computed as
the likelihood ratio of the likelihood function.

\section{Calibration Study}
In this calibration study, we simulate from the model with parameters
corresponding to the continuous-time version of the model, where we
pick the parameters as we did in the first paper.

\section{Application}
I will need a bit more guidance here.

\bibliographystyle{plainnat}
\bibliography{master-bibliography}

\end{document}