\documentclass[10pt]{article}

\usepackage{amsthm}
\usepackage[toc,page]{appendix}
\usepackage{amssymb}
\usepackage{pslatex,palatino,avant,graphicx,color}
\usepackage{colortbl}
\usepackage{fullpage}
\usepackage{mathtools}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{caption}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{enumerate}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{bbm}
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents

\newtheorem{lemma}{Lemma}
%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{0in}%
\addtolength{\evensidemargin}{0in}%
\addtolength{\textwidth}{0in}%
\addtolength{\textheight}{0in}%
\addtolength{\topmargin}{0in}%


\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

\newcommand{\indicator}[1]{\mathbbm{1}\left( #1 \right) }
\newcommand{\hb}{\hat{b}}
\newcommand{\ha}{\hat{a}}
\newcommand{\htheta}{\hat{\theta}}
\newcommand{\halpha}{\hat{\alpha}}
\newcommand{\hmu}{\hat{\mu}}
\newcommand{\hsigma}{\hat{\sigma}}
\newcommand{\hphi}{\hat{\phi}}
\newcommand{\htau}{\hat{\tau}}
\newcommand{\heta}{\hat{\eta}}
\newcommand{\E}[1]{\mbox{E}\left[#1\right]}
\newcommand{\Var}[1]{\mbox{Var}\left[#1\right]}
\newcommand{\Indicator}[1]{\mathbbm{1}_{ \left( #1 \right) } }
\newcommand{\dNormal}[3]{ N\left( #1 \left| #2, #3 \right. \right) }
\newcommand{\Beta}[2]{\mbox{Beta}\left( #1, #2 \right)}
\newcommand{\alphaphi}{\alpha_{\hphi}}
\newcommand{\betaphi}{\beta_{\hphi}}
\newcommand{\expo}[1]{ \exp\left\{ #1 \right\}}
\newcommand{\tauSquareDelta}{\htau^2
  \left(\frac{1-\expo{-2\htheta\Delta}}{2\htheta} \right)}
\newcommand{\mumu}{\mu_{\hmu}}
\newcommand{\sigmamu}{\sigma^2_{\hmu}}
\newcommand{\sigmamuexpr}{\log\left( \frac{\VarX}{\EX^2} + 1 \right)}
\newcommand{\mumuexpr}{\log(\EX) -  \log\left( \frac{\VarX}{\EX^2} + 1 \right) /2 }

\newcommand{\EX}{\mbox{E}\left[ X \right] }
\newcommand{\VarX}{\mbox{Var}\left[ X \right] }
\newcommand{\mueta}{\mu_{\heta} }
\newcommand{\sigmaeta}{\sigma^2_{\heta}}
\newcommand{\sigmaetaexpr}{ \log\left( \frac{\VarX}{\EX^2} + 1 \right) }
\newcommand{\muetaexpr}{ \log(\EX) -  \sigmaetaexpr /2 }

\newcommand{\mualpha}{\mu_{\halpha} }
\newcommand{\sigmaalpha}{\sigma^2_{\halpha}}
\newcommand{\sigmaalphaexpr}{ \log\left( \frac{\VarX}{\EX^2} + 1 \right) }
\newcommand{\mualphaexpr}{ \log(\EX) -  \sigmaalphaexpr /2 }

\newcommand{\mutauexpr}{ \frac{2}{T} \EX }
\newcommand{\sigmatauexpr}{ \frac{4}{T^2} \Var{X}}

\newcommand{\alphatau}{\alpha_{\htau^2}}
\newcommand{\betatau}{\beta_{\htau^2}}

\newcommand{\Gam}[2]{\mbox{Gamma}\left( #1, #2 \right) }
\newcommand{\InvGam}[2]{\mbox{Inv-Gamma}\left( #1, #2 \right) }

%%% END Article customizations

%%% The "real" document content comes below...

% \newbox{\LegendeA}
% \savebox{\LegendeA}{
%    (\begin{pspicture}(0,0)(0.6,0)
%    \psline[linewidth=0.04,linecolor=red](0,0.1)(0.6,0.1)
%    \end{pspicture})}
% \newbox{\LegendeB}
%    \savebox{\LegendeB}{
%    (\begin{pspicture}(0,0)(0.6,0)
%    \psline[linestyle=dashed,dash=1pt 2pt,linewidth=0.04,linecolor=blue](0,0.1)(0.6,0.1)
%    \end{pspicture})}

\title{Solution to a Non-Seperable Diffusion Equation on a Regular Domain}
\author{Georgi Dinolov, Abel Rodriguez, Hongyun Wang}
\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

\bigskip

\vspace{1cm}
\noindent

\spacingset{1.00} % 
\section{Introduction}

We consider two-dimensional correlated Brownian motion with absorbing boundaries:
\begin{align}
  X(t) &= x_0 + \mu_x t + \sigma_x W_x(t) &a_x &< X(t) < b_x   \label{eq:X} \\
  Y(t) &= y_0 + \mu_y t + \sigma_y W_y(t) &a_y &< Y(t) < b_y   \label{eq:Y} 
\end{align}
where $W_i$ are standard Brownian motions with
$\mbox{Cov}(W_1(t), W_2(t)) = \rho t$ for $0 < t' \leq t$. In
particular, we find the joint transition density function for
$(X(t), Y(t))$ under the boundary conditions:
\begin{align}
  \Pr\left(X(t) \in dx, Y(t) \in dy | a_x < X(t') < b_x, a_y < Y(t') < b_y, 0 < t' \leq t, X(0)=x_0, Y(0)=y_0, \theta \right), \label{eq:CDF} 
\end{align}
with $\theta := (\mu_x, \mu_y, \sigma_x, \sigma_y, \rho).$ This
density, which we shorten to $q(x,y,t)$ from now on, is the solution
to the Fokker-Planck equation \citep{oksendal2013stochastic}:
\begin{align}
  \frac{\partial}{\partial t} q(x,y,t') &= -\mu_x \frac{\partial}{\partial x}q(x,y,t')
                                         - \mu_y \frac{\partial}{\partial y}q(x,y,t')
                                         + \frac{1}{2}\sigma_x^2 \frac{\partial^2}{\partial x^2}q(x,y,t')
                                         + \rho\sigma_x\sigma_y \frac{\partial^2}{\partial x \partial y}q(x,y,t')
                                         + \frac{1}{2}\sigma_y^2 \frac{\partial^2}{\partial y^2}q(x,y,t'),
  \label{eq:1} \\
  q(a_x, y,t') &= q(b_x,y,t') = q(x,a_y,t') = q(x,b_y,t') = 0, \label{eq:2} \\
   0 &< t' \leq t. \nonumber
\end{align}
Differentiating $q(x,y,t)$ with respect to the boundaries produces the
transition density of a particle beginning and ending at the points
$(X_1(0), X_2(0))$ and $(X_1(t), X_2(t))$, respectively, while
attaining the minima $a_x/a_y$ and maxima $b_x/b_y$ in each coordinate
direction:
\begin{align*}
  \frac{\partial^4}{\partial a_x \partial b_x \partial a_y \partial
  b_y} q(x,y,t) = 
\end{align*}
\begin{align}
  \Pr\left(X(t) \in dx, Y(t) \in dy \left| \min_{t'}X(t') = a_x,
  \max_{t'}X(t')=b_x, \min_{t'} Y(t')=a_y, \max_{t'} Y(t')=b_y, 0 <
  t' \leq t, X(0)=x_0, Y(0)=y_0, \theta \right.\right). \label{eq:pdf}
\end{align}
The transition density (\ref{eq:CDF}) with less than 4 boundaries has
been used in computing first passage times \citep{kou2016first,
  sacerdote2016first}, with application to structural models in credit
risk and default correlations \citep{haworth2008modelling,
  ching2014correlated}. \cite{he1998double} use variants of
(\ref{eq:pdf}) with respect to some of the boundaries to price
financial derivative instruments whose payoff depends on \textbf{some}
of the observed maxima/minima.

Closed-form solutions to (\ref{eq:1}) - (\ref{eq:2}) are available for
some parameter regimes. When $\rho = 0$, the transition density of the
process is the solution to a well-understood Sturm-Liouville problem
where the eigenfunctions of the differential operator are sine
functions. When $a_1 = -\infty$ and $b_1 = \infty$, the method of
images can be used to enforce the remaining boundaries. For either
$a_1, a_2 = -\infty$ or $b_1, b_2 = \infty$, eigenfunction of the
Fokker-Plank equation can be found in radial coordinates. Both of
these techniques are used and detailed by
\cite{he1998double}. However, to the best of our knowledge, there is
no closed-form solution to the general problem in (\ref{eq:1}) -
(\ref{eq:2}). This also limits the available ways to compute
(\ref{eq:pdf}), with the most straightforward approach being finite
difference with respect to the boundary conditions. This, however,
requires one to solve at least 16 eigenvalue problems to evaluate the
density function for a single observation, motivating the need for an
efficient numerical method to solve (\ref{eq:1}) - (\ref{eq:2}).

It is still possible to approach the general problem by proposing a
biorthogonal expansion in time and space
(\cite{risken1989fokker-planck}, sections 6.2), where the
eigenfunctions for the differential operator are approximated as
sinusoidal series satisfying the boundary conditions. However, a
drawback of this out-of-the-box solution is that the system matrix for
the corresponding eigenvalue problem is large and dense. An
alternative is to use a finite difference scheme to directly solve the
evolution problem after suitable transformations. However, both of
these methods need a high degree of numerical resolution to produce
practically useful approximations of the transition density. We
conjecture that these inefficiencies come from either using a
\textit{separable} representation for the differential operator
(trigonometric series) or introducing numerical diffusion (finite
difference).

In this paper, we propose a solution to the general problem
(\ref{eq:1}) - (\ref{eq:2}) which is obtained by combining a
small-time analytic solution with a finite-element method. Our method
directly takes into account the correlation parameter present in the
differential operator in order to efficiently represent the analytic
small-time solution and propagate it forward in time. We apply our
computational method to estimate equation parameters with a maximum
likelihood approach in settings where the model assumptions of
constant $(\mu_x, \mu_y, \sigma_x, \sigma_y, \rho)$ and Brownian
motion driving stochastic evolution are appropriate.


\section{Approximate Numerical Solutions}
Before considering any solutions to the full Fokker-Planck equation
(\ref{eq:1}) - (\ref{eq:2}), we simplify the PDE by proposing an
exponential decomposition of the solution and using the fact that parameters
$(\mu_x, \mu_y, \sigma_x, \sigma_y, \rho)$ are constant:
\[
  q(x,y,t) = \exp(\alpha x + \beta y + \gamma t) p(x,y,t).
\]
We can find $\alpha, \beta$ and $\gamma$, as well as a suitable
scaling transformation, such that $p(\xi,\eta,\tau)$ satisfies the diffusion
equation:
\begin{align}
  \frac{\partial}{\partial \tau} p(\xi,\eta,\tau') &= \frac{1}{2}\sigma_\xi^2
  \frac{\partial^2}{\partial \xi^2}p(\xi,\eta,\tau') + \rho\sigma_\xi\sigma_\eta
  \frac{\partial^2}{\partial \xi \partial \eta}p(\xi,\eta,\tau') +
                                                     \frac{1}{2}\sigma_\eta^2 \frac{\partial^2}{\partial \eta^2}p(\xi,\eta,\tau') , \label{eq:qq} \\
  &:= \mathcal{L}p(\xi,\eta,\tau) , \\
  p(\xi,\eta,\tau) &=0 &\mbox{for } & (\xi,\eta) \in [0,1] \times [0,1], \nonumber \\
  p(\xi,\eta,0) &= \delta(\xi-\xi_0) \delta(\eta-\eta_0) \nonumber
\end{align}
on the unit square.  The transformations $\tilde{\theta} \to \theta$
as well as $(\xi,\eta) \to (x,y)$ allow us to go from
$p(\xi,\eta,\tau)$ to $p(x,y,t)$ without trouble. Note here that under
this transformation $\rho$ remains the same as in the original
coordinate frame. We will call equation (\ref{eq:qq}) the
\textit{normalized} problem and will consider its solution without
loss of generality.

\subsection{Eigenfunction Expansion} \label{sec:eigenfunction}
Following Section 6.2 of \cite{risken1989fokker-planck}, we may use
the biorthogonal decomposition of the solution as a sum of
eigenfunctions and time-dependent coefficients determined by eigenvalues:
\begin{equation}
  p(\xi,\eta,\tau) = \phi_\nu(\xi,\eta) e^{-\lambda_\nu \tau}, \label{eq:biorthogonal}
\end{equation}
where the eigenfunctions $\phi_\nu(\xi, \eta)$ satisfy the boundary
conditions. Because the differential operator in the normalized
problem (\ref{eq:qq}) is self-adjoint [PROVE], the family of
eigenfunctions is complete in the Hilbert space $L^2$
[CITE]. Moreover, the eigenvalues are bounded below by 0, so
that the solution behaves as expected (see section 6.3 of
\cite{risken1989fokker-planck}).

Since we require $\phi_\nu(\xi,\eta)$ to be zero on the boundaries, we
may represent the eigenfunction as a linear combination of sines
\[
  \phi_\nu(\xi,\eta) = \sum_{l=0}^L \sum_{m=0}^M c_{l,m, \nu}
  \sin\left(2\pi\, l\, \xi \right) \sin\left(2\pi\, m\, \eta \right) := \Psi(\xi,\eta)^T c_\nu,
\]
where we have truncated the infinite series for some suitably large
$L$ and $M$ and defined
\begin{align*}
  \psi_{l,m}(\xi,\eta) &= \sin\left(2\pi\, l\, \xi \right)
                         \sin\left(2\pi\, m\, \eta \right), \\
  \Psi(\xi,\eta) &= (\psi_{0,0}(\xi,\eta), \ldots, \psi_{L,M}(\xi,\eta))^T, \\
  c_\nu &= (c_{0,0,\nu}, \ldots, c_{L,M,\nu})^T.
\end{align*}

The biorthogonal representation (\ref{eq:biorthogonal}) leads to the
eigenvalue problem
\begin{equation}
  \mathcal{L} \phi_\nu = -\lambda_\nu \phi_\nu, \label{eq:eigenproblem}
\end{equation}
where $\mathcal{L}$ is the differential operator in the normalized
Fokker-Planck equation. Applying $\mathcal{L}$ to $\phi_\nu$ produces
the linear system
\[
  \mathcal{L}\phi_\nu = \mathcal{L}(\Psi(\xi,\eta)^T c_\nu) =
  \mathcal{L}(\Psi(\xi,\eta)^T) c_\nu = (A \Psi(\xi,\eta))^T c_\nu,
\] 
where $A$ is a constant matrix dependent on $\tilde{\theta}$. In the
case where $\rho = 0$, $A$ is diagonal because
$\left\{ \psi_{l,m}(\xi,\eta) \right\}_{l,m}$ are the eigenfunctions to
$\mathcal{L}$. When $\rho \neq 0$, $A$ is no longer diagonal and is in
fact dense. This caused by the mixing terms
\[
  \frac{\partial^2}{\partial \xi \partial \eta} \sin\left(2\pi\, l\,
    \xi\right) \sin\left(2\pi\, m\, \eta\right) = (2\pi\, l)(2\pi\, m)
  \cos\left(2\pi\, l\, \xi\right) \cos\left(2\pi\, m\, \eta\right)
\]
being the products of cosine functions, which have an inefficient sine
series representation [CITE]. Substituting the linear representation
of $\mathcal{L}\phi_\nu$ into the eigenvalue problem
(\ref{eq:eigenproblem}), we arrive to the system
\[
  \Psi(\xi,\eta)^T A^T c_\nu = -\lambda_\nu \Psi(\xi,\eta)^T c_\nu
  \quad \Leftrightarrow \quad A^T c_\nu = -\lambda_\nu c_\nu
\]
whose solution gives the family of orthonormal eigenfunctions. As
mentioned already, the efficiency of this approach is dependent on the
cost of solving the eigenvalue problem
$A^T c_\nu = -\lambda_\nu c_\nu$. 

\subsection{Finite Difference}  \label{sec:finite-difference}
A finite difference method used to solve the problem (\ref{eq:qq})
defines an approximate solution over some grid of points
$\left\{(\xi_l, \eta_m) \right\}_{l=0,m=0}^{L,M}$ over $[0,1] \times [0,1]:$
\begin{equation}
  q(\xi,\eta,\tau) \approx \sum_{l}\sum_{m} c_{l,m}(\tau) \delta(\xi-\xi_l) \delta(\eta-\eta_m) = \Delta(\xi,\eta)^T c(t),\label{eq:finite-diff-sol}
\end{equation}
where $c(\tau) = (c_{0,0}(\tau), \ldots, c_{L,M}(\tau)^T$ and
$\Delta(\xi,\eta) = (\delta(\xi-\xi_0) \delta(\eta-\eta_0), \ldots,
\delta(\xi-\xi_L) \delta(\eta-\eta_M))$, where we have once again
separated the spatial and temporal components of the problem as in the
previous section. This is a suitable choice, because the differential
operator $\mathcal{L}$ is linear and constant and there is therefore
no need to perform approximation in time, i.e. we take derivative with
respect to time directly:
\[
  \frac{\partial}{\partial \tau}q(\xi,\eta,\tau) \approx \Delta(\xi,\eta)^T \frac{\partial c(\tau)}{\partial \tau}
\]
The differential operator $\mathcal{L}$ for the representation in
equation (\ref{eq:finite-diff-sol}) is approximated with a finite
difference operator $\mathcal{L}_{FD}$ such that approximate derivatives are defined on the grid:
\[
  \mathcal{L}q(\xi,\eta,\tau) \approx \mathcal{L}_{FD}q(\xi,\eta,\tau) = \sum_{l}\sum_{m} f(c_{l,m}(\tau)) \delta(\xi-\xi_l) \delta(\eta-\eta_m) := \Delta(\xi,\eta)^T f(c(t))
\]
where $f(c_{l,m}(\tau))$ is a function of some neighboring coefficient
values at $(\xi_l, \eta_m)$.

For a central difference scheme on a regular $N \times N$ grid aligned
with the boundaries (with step size $h=1/(N-1)$), 
\[
  \frac{\partial^2}{\partial \xi^2} q(\xi_l, \eta_m, \tau) \approx
  \frac{c_{l+1,m}(\tau) - 2c_{l,m}(\tau) + c_{l-1,m}(\tau)}{h^2} = \frac{1}{h^2}A_{l,m,\xi^2} c(\tau),
\]
where $A_{l,m,\xi^2}$ is some all-zero row vector except for three
entries of 1 corresponding to the grid points
$(\xi_{l-1}, \eta_m), (\xi_l,\eta_m), (\xi_{l+1},\eta_m)$. For the
mixing term, the approximation is
\[
  \frac{\partial^2}{\partial \xi \partial \eta} q(\xi_l, \eta_m, \tau) \approx
  \frac{c_{l+1,m+1}(\tau) - c_{l+1,m-1}(\tau) - c_{l-1,m+1}(\tau) + c_{l-1,m-1}(\tau)}{4h^2} = \frac{1}{4h^2}A_{l,m,\xi\eta} c(\tau).
\]
The finite difference approximation of $\mathcal{L}$ can be written as
a linear transformation of $c(\tau)$:
\[
  \mathcal{L}q(\xi,\eta,\tau) \approx \Delta^T(\xi,\eta) \underbrace{\left( \frac{1}{2}\sigma_\xi^2 \frac{1}{h^2}A_{\xi^2} + \rho\sigma_\xi\sigma_\eta A_{\xi\eta} + \frac{1}{2}\sigma_\xi^2 \frac{1}{h^2}A_{\eta^2}  \right)}_{A} c(\tau),
\]
where we have composed the row vectors for the different derivative
terms as matrices $A_{\xi^2}, A_{\xi\eta},$ and $A_{\eta^2}$. The
system of differential equations for $c(\tau)$ is therefore completely
determined by our choice of step size $h$, as well as the parameter
values $(\sigma_\xi, \sigma_\eta, \rho)$:
\begin{align}
   &\frac{\partial c(\tau)}{\partial \tau} = A c(\tau) \nonumber \\
   \Rightarrow &c(\tau) = \exp\left( A\tau \right)c(0) \label{eq:eigenproblem-fd}
\end{align}
For non-small $\tau$, we must find the eigenvalue decomposition of $A$
in order to solve the evolution problem. It should be noted here that
a regular grid approach with a constant $h$ is appealing, because it
allows us to fill once and store the matrices
$A_{\xi^2}, A_{\eta^2}, A_{\xi\eta}$, which saves valuable
computational resources if we are to solve the finite difference
eigenproblem (\ref{eq:eigenproblem-fd}) repeatedly for different
parameter values $\tilde{\theta}$.

Unlike the system matrix for the trigonometric expansion, the system
matrix here is sparse, as was demonstrated for $A_{\xi^2}$ explicitly.
Eigenvalue problem is therefore much cheaper to solve. Further, the
system matrix can be made even sparser by performing a $45^{\circ}$
rotation which removes the mixing term from the problem PDE and still
allows us to use a regular grid.

However, the fundamental limitation of using a finite element method
is that differentiation with respect to boundaries is also done using
finite difference and it is of fourth order. Given this high order
differentiation, $h$ cannot be made too small because roundoff error
becomes an issue relatively quickly.

This naturally occuring lower bound on $h$ introduces yet another
practical issue: on a regular grid, the delta function intial
condition at $(\xi_0, \eta_0)$ needs to be either rounded to the
nearest grid point or represented as a weighted sum of delta functions
on the four nearest grid points. Either appraoch introduces a
numerical diffusion into the problem which, for a finite $h$, can bias
the numerical solution.

\subsection{Finite Element Method}
We propose a numerical method which $1)$ maintains a functional
approximation of the differential operator $\mathcal{L}$ like the
eigenfunction expansion in Section \ref{sec:eigenfunction} while $2)$
imposing a computational burded comparable to or better than that of
the finite difference approach in Section
\ref{sec:finite-difference}. Further, our method explicitly minimizes
the error associated with using a finite, smooth representation of the
initial condition $\delta(\xi-\xi_0)\delta(\eta-\eta_0)$.

As such, our approach consists of two parts
\begin{enumerate}[i)]
\item a small-time analytic solution $q(x,y,\tau_\epsilon)$ for the IC/BC problem,
\item a family of orthonormal basis functions which represent
  $q(x,y,\tau_\epsilon)$ parsimoniously.
\end{enumerate}
By combining $i)$ and $ii)$, we can efficiently find a weak solution to the
PDE (\ref{eq:qq}) via the finite element method
\citep{shaidurov2013multigrid}. Convergence of our method to the
strong solution under the $L^2(\bar{\Omega})$ norm is guaranteed as
long as the family we propose is complete in the Hilbert space of
functions induced under $L^2(\bar{\Omega})$ \citep{salsa2016partial}.

The small-time solution is derived by considering the fundamental
solution $G(\xi,\eta |\tau, \xi_0, \eta_0)$ for the unbounded problem
in (\ref{eq:qq}), which is the bivariate Gaussian density with mean
and covariance determined by the initial condition and the diffusion
parameters \citep{stakgold2011green}. We can then find a small enough
$t_\epsilon$ such that $G(\xi,\eta |\tau, \xi_0, \eta_0)$ is
numerically zero on three of the four boundaries of
$\bar{\Omega}$. The zero-condition on the remaining boundary is
enforced by suitably moving the source term for the fundamental solut
$(\xi'_0, \eta'_0)$. The small-time solution
therefore takes on the analytic form
\[
  q(\xi,\eta,\tau_\epsilon) = G(\xi,\eta|\tau_\epsilon,\xi_0, \eta_0) - G(\xi,\eta|\tau_\epsilon,\xi'_0, \eta'_0).
\]

The construction of the orthonormal basis functions is motivated by
the fundadmental solution for the unbounded problem (\ref{eq:qq}):
before performing Gram-Schmidt orthogonalization, the finite family of
basis functions
$\{ \tilde{\psi}_k(x,y| x_k, y_k, \rho, \sigma) \}_{k}^K$ are of the
form
\[
  \tilde{\psi}_k(x,y| x_k, y_k, \rho, \sigma) = N\left( (x,y)^T \left|
      (x_k, y_k)^T , \quad \left( \begin{array}{cc}
                                     \sigma^2 & \rho \sigma^2 \\
                                     \rho \sigma^2 & \sigma^2
                     \end{array} \right) \right. \right) x(1-x)y(1-y).
\]
Essentially, the collection
$\{ \tilde{\psi}_k(x,y| x_k, y_k, \rho, \sigma) \}_{k}^K$ is composed
of fundamental solutions to a heat diffusion problem tuned by $\sigma$
and $\rho$, mollified such that their support is on $\bar{\Omega}$,
and centered along some grid over $\bar{\Omega}$.

The advantage of these elements is that they better resolve the
fundamental solution for the unbounded problem by taking into account
$\rho$ in the covariance of each kernel. By performing Gram-Schmidt
orthogonalization under the $L^2(\Omega)$ norm, we arrive at a family
of orthonormal functions which can better resolve small-time solutions
having a large correlation coefficient. [Wow this needs a lot more
work].

% \subsection{Small-time Analytic Solution}
% The fundamental solution for the heat problem in (\ref{eq:qq})
% \textit{without the boundary conditions} is the bivariate Gaussian
% density function
% \begin{align}
%   G(x,y,t | x_0, y_0) &= N\left( (x,y)^T \left| (x_0, y_0)^T , \quad
%              t\left( \begin{array}{cc}
%                        \sigma_x^2 & \rho \sigma_x \sigma_y \\
%                        \rho \sigma_x \sigma_y & \sigma_y^2
%                      \end{array} \right) \right. \right).         
% \end{align}
% For a fixed $(x_0,y_0)$, we can find a small time $t_\epsilon$
% solution to the PDE where three of the four boundaries are enforced
% numerically (maximum value of numerical solution is $\sim 10^{-16}$,
% while the last boundary condition is enforced by reflecting $G(x,y,t)$
% accross it. This small-time solution is analytic, satisfying both the
% differential operator as well as the IC/BC. Let this small-time solution be denoted as
% \[
%   q(x,y,t_\epsilon).
% \]

% \subsection{Finite Element Method}
% We use the small-time solution as an initial condition in a finite
% element scheme to find the weak solution to (\ref{eq:qq}). To do so,
% we need to choose a suitable family of basis functions. Apart from
% satisfying the boundary conditions, there are many choices available
% for basis functions. The out-of-the-box solution is usually linear
% interpolants on either triangular or rectangular meshes
% \citep{shaidurov2013multigrid} over the computational
% domain. However, such an approach ignores the correlation structure
% of the solution to (\ref{eq:qq}).

% For this reason, we propose a family of basis functions which do
% take into account the correlation.

\section{Estimation}
As an application of our computational method, we estimate the
parameters in (\ref{eq:pdf}). However, before we do so, we prove a
lemma to show that maximum likelihood estimates based on the
approximate solution are asymptotically efficient.

\begin{lemma}
  The maximum likelihood estimator is consistent as $n \to \infty$ and $k \to \infty$:
  \[ \hat{\theta}_{n,k} \to \theta \].
\end{lemma}
\begin{proof}
  By the definition of weak convergence, given the weak solution $q_k$
  and the classical solution $q$, for any continuous function $f$,
  \[ \langle q_k| f \rangle \to \langle q| f \rangle \mbox { as } k
    \to \infty. \]
  Because $f$ can be any function in $L^2$, we can
  choose $f$ to be $\exp(ilx)$ for any integer $l$. This means that
  the characteristic function of $X_k$ converges pointwise to the
  characteristic function of $X$. By Levy's continuity theorem, this
  means that
  \[ X_k \xrightarrow[]{d} X \mbox { as } k \to \infty. \]

  Next, given Theorem 4.1 in \cite{singler2008differentiability}, we
  know that, for each $k$, $q_k$ satisfies the criteria A1 - A6 in 
  \cite{casella2002statistical} to guarantee that, for data
  $X_{k} \sim F_k(\theta)$,
  \[ \hat{\theta}_{n,k}(X_k) \xrightarrow[]{p} \theta \] as
  $n \to \infty$. Moreover, we are guaranteed asymptotic
  efficiency. In other words, the MLE estimator for
  $(\sigma_x, \sigma_y, \rho)$ based on the likelihood function under
  $F_k$ for data sampled from $F_k$ is asymptotically efficient. Now we need to show
  that the same holds for data sampled from $F$ as $k \to \infty$.

  To do this, we will use Chebyshev's inequality:
  \[
    \Pr_{X}\left( \left| \hat{\theta}_{n,k}(X) - \theta \right| \geq
      \epsilon \right) \leq \frac{ \mbox{E}_{X}\left[
        (\hat{\theta}_{n,k}(X) - \theta)^2 \right] }{ \epsilon^2 }.
  \]
  By the Maximum theorem, $\hat{\theta}_{n,k}(x)$ is a continuous
  function with respect to $x$, and further because we have bounded
  $\hat{\theta}$ from below and above,
  \[
    \mbox{E}_{X_k}\left[ (\hat{\theta}_{n,k}(X_k) - \theta)^2 \right]
    \to \mbox{E}_{X}\left[ (\hat{\theta}_{n,k}(X) - \theta)^2 \right]
    \mbox{ as } k \to \infty
  \]
  by the portmanteau lemma. Finally, because $\hat{\theta}_{k,n}$ is
  asymptotically efficient, we can show that
  \[
    \mbox{E}_{X_k}\left[ (\hat{\theta}_{n,k}(X_k) - \theta)^2 \right]
    \to 0 \mbox{ as } n \to \infty,
  \]
  since the expected value of the estimator tends to $\theta$ and its
  variance goes to 0 when $n \to \infty$. Therefore, given any
  $\epsilon > 0$ and $\delta > 0$, we can find a sufficiently large $n$ and $k$ such that
  \[
    \Pr_{X}\left( \left| \hat{\theta}_{n,k}(X) - \theta \right| \geq
      \epsilon \right) \leq \frac{ \mbox{E}_{X}\left[
        (\hat{\theta}_{n,k}(X) - \theta)^2 \right] }{ \epsilon^2 } < \delta    
  \]
\end{proof}


\bibliographystyle{plainnat}
\bibliography{master-bibliography}
\end{document}
