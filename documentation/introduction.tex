\documentclass[10pt]{article}

\usepackage{amsthm}
\usepackage[toc,page]{appendix}
\usepackage{amssymb}
\usepackage{pslatex,palatino,avant,graphicx,color}
\usepackage{colortbl}
\usepackage{fullpage}
\usepackage{mathtools}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{caption}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{enumerate}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{bbm}
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents

\newtheorem{lemma}{Lemma}
%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{0in}%
\addtolength{\evensidemargin}{0in}%
\addtolength{\textwidth}{0in}%
\addtolength{\textheight}{0in}%
\addtolength{\topmargin}{0in}%


\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

\newcommand{\indicator}[1]{\mathbbm{1}\left( #1 \right) }
\newcommand{\hb}{\hat{b}}
\newcommand{\ha}{\hat{a}}
\newcommand{\htheta}{\hat{\theta}}
\newcommand{\halpha}{\hat{\alpha}}
\newcommand{\hmu}{\hat{\mu}}
\newcommand{\hsigma}{\hat{\sigma}}
\newcommand{\hphi}{\hat{\phi}}
\newcommand{\htau}{\hat{\tau}}
\newcommand{\heta}{\hat{\eta}}
\newcommand{\E}[1]{\mbox{E}\left[#1\right]}
\newcommand{\Var}[1]{\mbox{Var}\left[#1\right]}
\newcommand{\Indicator}[1]{\mathbbm{1}_{ \left( #1 \right) } }
\newcommand{\dNormal}[3]{ N\left( #1 \left| #2, #3 \right. \right) }
\newcommand{\Beta}[2]{\mbox{Beta}\left( #1, #2 \right)}
\newcommand{\alphaphi}{\alpha_{\hphi}}
\newcommand{\betaphi}{\beta_{\hphi}}
\newcommand{\expo}[1]{ \exp\left\{ #1 \right\}}
\newcommand{\tauSquareDelta}{\htau^2
  \left(\frac{1-\expo{-2\htheta\Delta}}{2\htheta} \right)}
\newcommand{\mumu}{\mu_{\hmu}}
\newcommand{\sigmamu}{\sigma^2_{\hmu}}
\newcommand{\sigmamuexpr}{\log\left( \frac{\VarX}{\EX^2} + 1 \right)}
\newcommand{\mumuexpr}{\log(\EX) -  \log\left( \frac{\VarX}{\EX^2} + 1 \right) /2 }

\newcommand{\EX}{\mbox{E}\left[ X \right] }
\newcommand{\VarX}{\mbox{Var}\left[ X \right] }
\newcommand{\mueta}{\mu_{\heta} }
\newcommand{\sigmaeta}{\sigma^2_{\heta}}
\newcommand{\sigmaetaexpr}{ \log\left( \frac{\VarX}{\EX^2} + 1 \right) }
\newcommand{\muetaexpr}{ \log(\EX) -  \sigmaetaexpr /2 }

\newcommand{\mualpha}{\mu_{\halpha} }
\newcommand{\sigmaalpha}{\sigma^2_{\halpha}}
\newcommand{\sigmaalphaexpr}{ \log\left( \frac{\VarX}{\EX^2} + 1 \right) }
\newcommand{\mualphaexpr}{ \log(\EX) -  \sigmaalphaexpr /2 }

\newcommand{\mutauexpr}{ \frac{2}{T} \EX }
\newcommand{\sigmatauexpr}{ \frac{4}{T^2} \Var{X}}

\newcommand{\alphatau}{\alpha_{\htau^2}}
\newcommand{\betatau}{\beta_{\htau^2}}

\newcommand{\Gam}[2]{\mbox{Gamma}\left( #1, #2 \right) }
\newcommand{\InvGam}[2]{\mbox{Inv-Gamma}\left( #1, #2 \right) }

%%% END Article customizations

%%% The "real" document content comes below...

% \newbox{\LegendeA}
% \savebox{\LegendeA}{
%    (\begin{pspicture}(0,0)(0.6,0)
%    \psline[linewidth=0.04,linecolor=red](0,0.1)(0.6,0.1)
%    \end{pspicture})}
% \newbox{\LegendeB}
%    \savebox{\LegendeB}{
%    (\begin{pspicture}(0,0)(0.6,0)
%    \psline[linestyle=dashed,dash=1pt 2pt,linewidth=0.04,linecolor=blue](0,0.1)(0.6,0.1)
%    \end{pspicture})}

\title{Solution to a Non-Seperable Diffusion Equation on a Regular Domain}
\author{Georgi Dinolov, Abel Rodriguez, Hongyun Wang}
\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

\bigskip

\vspace{1cm}
\noindent

\spacingset{1.00} % 
\section{Introduction}

We consider two-dimensional correlated Brownian motion with absorbing boundaries:
\begin{align}
  X(t) &= x_0 + \mu_x t + \sigma_x W_x(t) &a_x &< X(t) < b_x   \label{eq:X} \\
  Y(t) &= y_0 + \mu_y t + \sigma_y W_y(t) &a_y &< Y(t) < b_y   \label{eq:Y} 
\end{align}
where $W_i$ are standard Brownian motions with
$\mbox{Cov}(W_1(t), W_2(t)) = \rho t$ for $0 < t' \leq t$. In
particular, we find the joint transition density function for
$(X(t), Y(t))$ under the boundary conditions:
\begin{align}
  \Pr\left(X(t) \in dx, Y(t) \in dy | \forall t' \in [0,t]\,\, X(t') \in [a_x, b_x], \froall t' \in [0,t]\,\, Y(t') \in [a_y,b_y], X(0)=x_0, Y(0)=y_0, \theta \right), \label{eq:CDF}
\end{align}
with $\theta := (\mu_x, \mu_y, \sigma_x, \sigma_y, \rho).$ This
function, which we shorten to $q(x,y,t)$ from now on, is the solution
to the Fokker-Planck equation \citep{oksendal2013stochastic}:
\begin{align}
  \frac{\partial}{\partial t} q(x,y,t') &= -\mu_x \frac{\partial}{\partial x}q(x,y,t')
                                         - \mu_y \frac{\partial}{\partial y}q(x,y,t')
                                         + \frac{1}{2}\sigma_x^2 \frac{\partial^2}{\partial x^2}q(x,y,t')
                                         + \rho\sigma_x\sigma_y \frac{\partial^2}{\partial x \partial y}q(x,y,t')
                                         + \frac{1}{2}\sigma_y^2 \frac{\partial^2}{\partial y^2}q(x,y,t'),
  \label{eq:1} \\
  q(a_x, y,t') &= q(b_x,y,t') = q(x,a_y,t') = q(x,b_y,t') = 0, \label{eq:2} \\
   0 &< t' \leq t. \nonumber
\end{align}
Differentiating $q(x,y,t)$ with respect to the boundaries produces the
transition density of a particle beginning and ending at the points
$(X_1(0), X_2(0))$ and $(X_1(t), X_2(t))$, respectively, while
attaining the minima $a_x/a_y$ and maxima $b_x/b_y$ in each coordinate
direction:
\begin{align*}
  \frac{\partial^4}{\partial a_x \partial b_x \partial a_y \partial
  b_y} q(x,y,t) = 
\end{align*}
\begin{align}
  \Pr\left(X(t) \in dx, Y(t) \in dy, \min_{t'}X(t') = a_x,
  \max_{t'}X(t')=b_x, \min_{t'} Y(t')=a_y, \max_{t'} Y(t')=b_y \left| 0 <
  t' \leq t, X(0)=x_0, Y(0)=y_0, \theta \right.\right). \label{eq:pdf}
\end{align}
The transition density (\ref{eq:CDF}) with less than all four
boundaries has been used in computing first passage times
\citep{kou2016first, sacerdote2016first}, with application to
structural models in credit risk and default correlations
\citep{haworth2008modelling, ching2014correlated}. \cite{he1998double}
use variants of (\ref{eq:pdf}) with respect to some of the boundaries
to price financial derivative instruments whose payoff depends on
\textbf{some} of the observed maxima/minima.

Closed-form solutions to (\ref{eq:1}) - (\ref{eq:2}) are available for
some parameter regimes. When $\rho = 0$, the transition density of the
process is the solution to a well-understood Sturm-Liouville problem
where the eigenfunctions of the differential operator are sine
functions. When $a_1 = -\infty$ and $b_1 = \infty$, the method of
images can be used to enforce the remaining boundaries. For either
$a_1, a_2 = -\infty$ or $b_1, b_2 = \infty$, eigenfunction of the
Fokker-Plank equation can be found in radial coordinates. Both of
these techniques are used and detailed by
\cite{he1998double}. However, to the best of our knowledge, there is
no closed-form solution to the general problem in (\ref{eq:1}) -
(\ref{eq:2}). This also limits the available ways to compute
(\ref{eq:pdf}), with the most straightforward approach being finite
difference with respect to the boundary conditions. This, however,
requires one to solve at least 16 eigenvalue problems to evaluate the
density function for a single observation, motivating the need for an
efficient numerical method to solve (\ref{eq:1}) - (\ref{eq:2}).

It is still possible to approach the general problem by proposing a
biorthogonal expansion in time and space
(\cite{risken1989fokker-planck}, sections 6.2), where the
eigenfunctions for the differential operator are approximated as
sinusoidal series satisfying the boundary conditions. However, a
drawback of this out-of-the-box solution is that the system matrix for
the corresponding eigenvalue problem is large and dense. An
alternative is to use a finite difference scheme to directly solve the
evolution problem after suitable transformations. However, both of
these methods need a high degree of numerical resolution to produce
practically useful approximations of the transition density. We
conjecture that these inefficiencies come from either using a
\textit{separable} representation for the differential operator
(trigonometric series) or introducing numerical diffusion (finite
difference).

In this paper, we propose a solution to the general problem
(\ref{eq:1}) - (\ref{eq:2}) which is obtained by combining a
small-time analytic solution with a finite-element method. Our method
directly takes into account the correlation parameter present in the
differential operator in order to efficiently represent the analytic
small-time solution and propagate it forward in time. We apply our
computational method to estimate equation parameters with a maximum
likelihood approach in settings where the model assumptions of
constant $(\mu_x, \mu_y, \sigma_x, \sigma_y, \rho)$ and Brownian
motion driving stochastic evolution are appropriate.


\section{Approximate Numerical Solutions}
Before considering any solutions to the full Fokker-Planck equation
(\ref{eq:1}) - (\ref{eq:2}), we simplify the PDE by proposing an
exponential decomposition of the solution and using the fact that parameters
$\theta = (\mu_x, \mu_y, \sigma_x, \sigma_y, \rho)$ are constant:
\[
  q(x,y,t) = \exp(\alpha x + \beta y + \gamma t) p(x,y,t).
\]
We can find $\alpha, \beta$ and $\gamma$, as well as a suitable
scaling transformation, such that $p(\xi,\eta,\tau)$ satisfies the diffusion
equation:
\begin{align}
  \frac{\partial}{\partial \tau} p(\xi,\eta,\tau') &= \frac{1}{2}\sigma_\xi^2
  \frac{\partial^2}{\partial \xi^2}p(\xi,\eta,\tau') + \rho\sigma_\xi\sigma_\eta
  \frac{\partial^2}{\partial \xi \partial \eta}p(\xi,\eta,\tau') +
                                                     \frac{1}{2}\sigma_\eta^2 \frac{\partial^2}{\partial \eta^2}p(\xi,\eta,\tau') , \label{eq:qq} \\
  &:= \mathcal{L}p(\xi,\eta,\tau) , \\
  p(\xi,\eta,\tau) &=0 &\mbox{for } & (\xi,\eta) \in [0,1] \times [0,1], \nonumber \\
  p(\xi,\eta,0) &= \delta(\xi-\xi_0) \delta(\eta-\eta_0) \nonumber
\end{align}
on the unit square.  The transformations $\tilde{\theta} \to \theta$
as well as $(\xi,\eta) \to (x,y)$ allow us to go from
$p(\xi,\eta,\tau)$ to $p(x,y,t)$ without trouble. Note here that under
this transformation $\rho$ remains the same as in the original
coordinate frame. We will call equation (\ref{eq:qq}) the
\textit{normalized} problem and will consider its solution without
loss of generality.

\subsection{Eigenfunction Expansion} \label{sec:eigenfunction}
Following Section 6.2 of \cite{risken1989fokker-planck}, we may use
the biorthogonal decomposition of the solution as a sum of
eigenfunctions and time-dependent coefficients determined by eigenvalues:
\begin{equation}
  p(\xi,\eta,\tau) = \phi_\nu(\xi,\eta) e^{-\lambda_\nu \tau}, \label{eq:biorthogonal}
\end{equation}
where the eigenfunctions $\phi_\nu(\xi, \eta)$ satisfy the boundary
conditions. Because the differential operator in the normalized
problem (\ref{eq:qq}) is self-adjoint [PROVE], the family of
eigenfunctions is complete in the Hilbert space $L^2$
[CITE]. Moreover, the eigenvalues are bounded below by 0, so
that the solution behaves as expected (see section 6.3 of
\cite{risken1989fokker-planck}).

Since we require $\phi_\nu(\xi,\eta)$ to be zero on the boundaries, we
may represent the eigenfunction as a linear combination of sines
\[
  \phi_\nu(\xi,\eta) = \sum_{l=0}^L \sum_{m=0}^M c_{l,m, \nu}
  \sin\left(2\pi\, l\, \xi \right) \sin\left(2\pi\, m\, \eta \right) := \Psi(\xi,\eta)^T c_\nu,
\]
where we have truncated the infinite series for some suitably large
$L$ and $M$ and defined
\begin{align*}
  \psi_{l,m}(\xi,\eta) &= \sin\left(2\pi\, l\, \xi \right)
                         \sin\left(2\pi\, m\, \eta \right), \\
  \Psi(\xi,\eta) &= (\psi_{0,0}(\xi,\eta), \ldots, \psi_{L,M}(\xi,\eta))^T, \\
  c_\nu &= (c_{0,0,\nu}, \ldots, c_{L,M,\nu})^T.
\end{align*}

The biorthogonal representation (\ref{eq:biorthogonal}) leads to the
eigenvalue problem
\begin{equation}
  \mathcal{L} \phi_\nu = -\lambda_\nu \phi_\nu, \label{eq:eigenproblem}
\end{equation}
where $\mathcal{L}$ is the differential operator in the normalized
Fokker-Planck equation. Applying $\mathcal{L}$ to $\phi_\nu$ produces
the linear system
\[
  \mathcal{L}\phi_\nu = \mathcal{L}(\Psi(\xi,\eta)^T c_\nu) =
  \mathcal{L}(\Psi(\xi,\eta)^T) c_\nu = (A \Psi(\xi,\eta))^T c_\nu,
\] 
where $A$ is a constant matrix dependent on $\tilde{\theta}$. In the
case where $\rho = 0$, $A$ is diagonal because
$\left\{ \psi_{l,m}(\xi,\eta) \right\}_{l,m}$ are the eigenfunctions to
$\mathcal{L}$. When $\rho \neq 0$, $A$ is no longer diagonal and is in
fact dense. This caused by the mixing terms
\[
  \frac{\partial^2}{\partial \xi \partial \eta} \sin\left(2\pi\, l\,
    \xi\right) \sin\left(2\pi\, m\, \eta\right) = (2\pi\, l)(2\pi\, m)
  \cos\left(2\pi\, l\, \xi\right) \cos\left(2\pi\, m\, \eta\right)
\]
being the products of cosine functions, which have an inefficient sine
series representation [CITE]. Substituting the linear representation
of $\mathcal{L}\phi_\nu$ into the eigenvalue problem
(\ref{eq:eigenproblem}), we arrive to the system
\[
  \Psi(\xi,\eta)^T A^T c_\nu = -\lambda_\nu \Psi(\xi,\eta)^T c_\nu
  \quad \Leftrightarrow \quad A^T c_\nu = -\lambda_\nu c_\nu
\]
whose solution gives the family of orthonormal eigenfunctions. As
mentioned already, the efficiency of this approach is dependent on the
cost of solving the eigenvalue problem
$A^T c_\nu = -\lambda_\nu c_\nu$. 

\subsection{Finite Difference}  \label{sec:finite-difference}
A finite difference method used to solve the problem (\ref{eq:qq})
defines an approximate solution over some grid of points
$\left\{(\xi_l, \eta_m) \right\}_{l=0,m=0}^{L,M}$ over $[0,1] \times [0,1]:$
\begin{equation}
  q(\xi,\eta,\tau) \approx \sum_{l}\sum_{m} c_{l,m}(\tau) \delta(\xi-\xi_l) \delta(\eta-\eta_m) = \Delta(\xi,\eta)^T c(t),\label{eq:finite-diff-sol}
\end{equation}
where $c(\tau) = (c_{0,0}(\tau), \ldots, c_{L,M}(\tau)^T$ and
$\Delta(\xi,\eta) = (\delta(\xi-\xi_0) \delta(\eta-\eta_0), \ldots,
\delta(\xi-\xi_L) \delta(\eta-\eta_M))$, where we have once again
separated the spatial and temporal components of the problem as in the
previous section. This is a suitable choice, because the differential
operator $\mathcal{L}$ is linear and constant and there is therefore
no need to perform approximation in time, i.e. we take derivative with
respect to time directly:
\[
  \frac{\partial}{\partial \tau}q(\xi,\eta,\tau) \approx \Delta(\xi,\eta)^T \frac{\partial c(\tau)}{\partial \tau}
\]
The differential operator $\mathcal{L}$ for the representation in
equation (\ref{eq:finite-diff-sol}) is approximated with a finite
difference operator $\mathcal{L}_{FD}$ such that approximate derivatives are defined on the grid:
\[
  \mathcal{L}q(\xi,\eta,\tau) \approx \mathcal{L}_{FD}q(\xi,\eta,\tau) = \sum_{l}\sum_{m} f(c_{l,m}(\tau)) \delta(\xi-\xi_l) \delta(\eta-\eta_m) := \Delta(\xi,\eta)^T f(c(t))
\]
where $f(c_{l,m}(\tau))$ is a function of some neighboring coefficient
values at $(\xi_l, \eta_m)$.

For a central difference scheme on a regular $N \times N$ grid aligned
with the boundaries (with step size $h=1/(N-1)$), 
\[
  \frac{\partial^2}{\partial \xi^2} q(\xi_l, \eta_m, \tau) \approx
  \frac{c_{l+1,m}(\tau) - 2c_{l,m}(\tau) + c_{l-1,m}(\tau)}{h^2} = \frac{1}{h^2}A_{l,m,\xi^2} c(\tau),
\]
where $A_{l,m,\xi^2}$ is some all-zero row vector except for three
entries of 1 corresponding to the grid points
$(\xi_{l-1}, \eta_m), (\xi_l,\eta_m), (\xi_{l+1},\eta_m)$. For the
mixing term, the approximation is
\[
  \frac{\partial^2}{\partial \xi \partial \eta} q(\xi_l, \eta_m, \tau) \approx
  \frac{c_{l+1,m+1}(\tau) - c_{l+1,m-1}(\tau) - c_{l-1,m+1}(\tau) + c_{l-1,m-1}(\tau)}{4h^2} = \frac{1}{4h^2}A_{l,m,\xi\eta} c(\tau).
\]
The finite difference approximation of $\mathcal{L}$ can be written as
a linear transformation of $c(\tau)$:
\[
  \mathcal{L}q(\xi,\eta,\tau) \approx \Delta^T(\xi,\eta) \underbrace{\left( \frac{1}{2}\sigma_\xi^2 \frac{1}{h^2}A_{\xi^2} + \rho\sigma_\xi\sigma_\eta A_{\xi\eta} + \frac{1}{2}\sigma_\xi^2 \frac{1}{h^2}A_{\eta^2}  \right)}_{A} c(\tau),
\]
where we have composed the row vectors for the different derivative
terms as matrices $A_{\xi^2}, A_{\xi\eta},$ and $A_{\eta^2}$. The
system of differential equations for $c(\tau)$ is therefore completely
determined by our choice of step size $h$, as well as the parameter
values $(\sigma_\xi, \sigma_\eta, \rho)$:
\begin{align}
   &\frac{\partial c(\tau)}{\partial \tau} = A c(\tau) \nonumber \\
   \Rightarrow &c(\tau) = \exp\left( A\tau \right)c(0) \label{eq:eigenproblem-fd}
\end{align}
For non-small $\tau$, we must find the eigenvalue decomposition of $A$
in order to solve the evolution problem. It should be noted here that
a regular grid approach with a constant $h$ is appealing, because it
allows us to construct once and store the matrices
$A_{\xi^2}, A_{\eta^2}, A_{\xi\eta}$, which saves valuable
computational resources if we are to solve the finite difference
eigenproblem (\ref{eq:eigenproblem-fd}) repeatedly for different
parameter values $\tilde{\theta}$.

Unlike the system matrix for the trigonometric expansion, the system
matrix here is sparse, as was demonstrated for $A_{\xi^2}$ explicitly.
The eigenvalue problem is therefore much cheaper to solve. The system
matrix can be made even sparser on a regular grid by performing a
$45^{\circ}$ rotation which removes the mixing term from the problem
PDE and preserves the boundaries.

However, the fundamental limitation of using a finite difference
method is that differentiation with respect to boundaries is also done
using finite differences and it is of fourth order. Given this high
order differentiation, $h$ cannot be made too small because roundoff
error becomes an issue relatively quickly. [\textbf{\color{red} this needs to be a
  bit more clear. Why isn't this an issue with a smooth
  approximation?}] This naturally occurring lower bound on $h$ introduces yet another
practical concerng: on a regular grid, the delta function initial
condition at $(\xi_0, \eta_0)$ needs to be either rounded to the
nearest grid point or represented as a weighted sum of delta functions
on the four nearest grid points. Either approach introduces a
numerical diffusion into the problem which, for a finite $h$, can bias
the numerical solution. [\textbf{\color{red} for which we have results}]

\subsection{Finite Element Method}
We propose a numerical method which $1)$ maintains a functional
approximation of the differential operator $\mathcal{L}$ while $2)$
imposing a computational burded comparable to or better than that of
the finite difference approach. Further, our method explicitly minimizes
the error associated with using a finite, smooth representation of the
initial condition $\delta(\xi-\xi_0)\delta(\eta-\eta_0)$.

As such, our approach consists of two parts
\begin{enumerate}[i)]
\item a small-time analytic solution $q(x,y,\tau_\epsilon)$ for the IC/BC problem,
\item a family of orthonormal basis functions which represent
  $q(x,y,\tau_\epsilon)$ parsimoniously.
\end{enumerate}
By combining $i)$ and $ii)$, we can efficiently find a weak solution
to the PDE (\ref{eq:qq}) via the finite element method
\citep{shaidurov2013multigrid}. Convergence of our method to the
strong solution under the $L^2(\bar{\Omega})$ norm is guaranteed as
long as the family of basis functions we propose is complete in the
Hilbert space under $L^2(\bar{\Omega})$ \citep{salsa2016partial}.

The small-time solution is derived by considering the fundamental
solution $G(\xi,\eta |\tau, \xi_0, \eta_0)$ for the unbounded problem
in (\ref{eq:qq}), which is the bivariate Gaussian density with mean
and covariance determined by the initial condition and the diffusion
parameters \citep{stakgold2011green}. We can then find a small enough
$t_\epsilon$ such that $G(\xi,\eta |\tau, \xi_0, \eta_0)$ is
numerically zero on three of the four boundaries of
$\bar{\Omega}$. The zero-condition on the remaining boundary is
enforced by suitably moving the source term for the fundamental
solution to some suitable $(\xi'_0, \eta'_0)$. The small-time solution
therefore takes on the analytic form
\[
  q(\xi,\eta,\tau_\epsilon) = G(\xi,\eta|\tau_\epsilon,\xi_0, \eta_0) - G(\xi,\eta|\tau_\epsilon,\xi'_0, \eta'_0).
\]

The construction of the orthonormal basis functions is also motivated
by the fundadmental solution for the unbounded problem (\ref{eq:qq}):
before performing Gram-Schmidt orthogonalization, the finite family of
basis functions
$\{ \tilde{\psi}_k(x,y| x_k, y_k, \rho, \sigma) \}_{k}^K$ are of the
form
\[
  \tilde{\psi}_k(x,y| x_k, y_k, \rho, \sigma) = N\left( (x,y)^T \left|
      (x_k, y_k)^T , \quad \left( \begin{array}{cc}
                                     \sigma^2 & \rho \sigma^2 \\
                                     \rho \sigma^2 & \sigma^2
                     \end{array} \right) \right. \right) x(1-x)y(1-y).
\]
Essentially, the collection
$\{ \tilde{\psi}_k(x,y| x_k, y_k, \rho, \sigma) \}_{k}^K$ is composed
of fundamental solutions to a heat diffusion problem tuned by $\sigma$
and $\rho$, tapered such that their support is on $\bar{\Omega}$,
and centered along some grid over $\bar{\Omega}$ and are still smooth.

The advantage of these elements is that they better resolve the
fundamental solution for the unbounded problem by taking into account
$\rho$ in the covariance of each kernel [\textbf{\color{red} if not
  prove, demonstrate}]. By performing Gram-Schmidt orthogonalization
under the $L^2(\Omega)$ norm, we arrive at a family of orthonormal
functions $\{ \psi_k(x,y| x_k, y_k, \rho, \sigma) \}_{k}^K$ which can
better resolve small-time solutions having a large correlation
coefficient. [This needs a lot more work].

\section{Estimation}
Our numerical method is specifically designed for computational
efficiency for repeatedly evaluating the density function
(\ref{eq:pdf}). The method is therefore particularly suited for
performing maximum likelihood estimation of parameter values with
respect to models involving systems of the type (\ref{eq:X}) -
(\ref{eq:Y}). In such a scenario, the maximum likelihood estimator
(MLE) for the true parameters $\theta$ are those which maximize the
(log-)likelihood function conditional on the observed data. MLEs are
especially useful in practical settings when they exhibit
\textit{consistency}, \textit{i.e.}: the MLEs gets closer to the true
parameter $\theta$ as more data is collected and included in the
likelihood (assuming the model and its parameters remain constant).

In this section, we prove that the MLE based on the
\textbf{approximate} (weak) finite element solution to the governing
Fokker-Planck equation (\ref{eq:1}) is consistent. We define $X$ as
the random variable sampled from the distribution corresponding to the
true density (\ref{eq:pdf}):
\[
  X \sim F(\theta),
\]
where we have explicitly made the density $F$ dependent on the model
parameters $\theta$.

For a family of basis functions
$\{ \tilde{\psi}_l(x,y| x_k, y_k, \rho, \sigma) \}_{l=0}^k$, let
$q_k(x,y,t|\theta)$ be the weak solution to the governing
Fokker-Planck equation (\ref{eq:1}) - (\ref{eq:2}). The approximation
to the true pdf is then
\[
  \frac{\partial^4}{\partial a_x \partial b_x \partial a_y \partial
    b_y} q_k(x,y,t).
\]
The distribution corresponding to this density is denoted as
$F_k(\theta)$. Let $X_k$ be the random variable corresponding to this
distribution:
\[
  X_k \sim F_k(\theta).
\]
We prove Lemma \ref{lem:1}, which states that the distribution of
random variables sampled according to the approximate solution
converge in density to random variables sampled from the true
solution:
\begin{lemma} \label{lem:1}
  For $X_k$ and $X$ defined above, $X_k \xrightarrow[]{d} X \mbox{ as } k \to \infty.$
\end{lemma}
\begin{proof}
  \textbf{\color{red}do not read below; I'm working on it tonight}
  The weak solution $q_k(x,y,t)$ has the property that for any
  $C^{\infty}(\Omega)$ function $v \in L^2(\Omega)$,
  \[ \underset{k \to \infty}{\lim}(q_k,v) = (q,v), \] where
  $(v,u) := \displaystyle \int_{a_x}^{b_x} \int_{a_y}^{b_y}
  v(x,y,t)u(x,y,t) dx\,dy.$ and $q$ is the strong solution.

  Recalling equation (\ref{eq:CDF}), the strong solution has the probabilistic interpretation
  \begin{align*}
    q(x,y,t) &= \Pr\left(X(t) \in dx, Y(t) \in dy, a_x < X(t') < b_x, a_y < Y(t') < b_y |0 < t' \leq t, X(0)=x_0, Y(0)=y_0, \theta \right) \\
     &= \Pr\left(X(t) \in dx, Y(t) \in dy, \min_{t'}X(t') \geq a_x,
  \max_{t'}X(t') \leq b_x, \min_{t'} Y(t') \geq a_y, \max_{t'} Y(t') \leq b_y \left| 0 <
  t' \leq t, X(0)=x_0, Y(0)=y_0, \theta \right.\right)
  \end{align*}
\end{proof}


\begin{lemma}
  The maximum likelihood estimator is consistent as $n \to \infty$ and $k \to \infty$:
  \[ \hat{\theta}_{n,k} \to \theta \].
\end{lemma}
\begin{proof}
  \textbf{\color{red} It's a bit sloppy. Also, we do not need
    asymptotic efficiency to prove equation (\ref{eq:var-lim})} By
  Lemma \ref{lem:1}
    \[ X_k \xrightarrow[]{d} X \mbox { as } k \to \infty. \] Next,
    given Theorem 4.1 in \cite{singler2008differentiability}, we know
    that, for each $k$, $q_k$ is analytic in both the diffusion
    parameters and boundary parameters. Hence, the probability density
    function satisfies the criteria A1 - A6 in
    \cite{casella2002statistical} to guarantee that, for data
    $X_{k} \sim F_k(\theta)$,
    \[ \hat{\theta}_{n,k}(X_k) \xrightarrow[]{p} \theta \mbox{ as } n
      \to \infty. \]

    Now we need to show that the same holds for data sampled from $F$
    as $k \to \infty$. To do this, we will use Chebyshev's inequality:
  \[
    \Pr_{X}\left( \left| \hat{\theta}_{n,k}(X) - \theta \right| \geq
      \epsilon \right) \leq \frac{ \mbox{E}_{X}\left[
        (\hat{\theta}_{n,k}(X) - \theta)^2 \right] }{ \epsilon^2 }.
  \]
  By the Maximum theorem [REFERENCE], $\hat{\theta}_{n,k}(x)$ is a continuous
  function with respect to $x$, and further because we have bounded
  $\hat{\theta}$ from below and above,
  \[
    \mbox{E}_{X_k}\left[ (\hat{\theta}_{n,k}(X_k) - \theta)^2 \right]
    \to \mbox{E}_{X}\left[ (\hat{\theta}_{n,k}(X) - \theta)^2 \right]
    \mbox{ as } k \to \infty
  \]
  by the portmanteau lemma. Finally, we can show that
  \begin{equation}
    \mbox{E}_{X_k}\left[ (\hat{\theta}_{n,k}(X_k) - \theta)^2 \right]
    \to 0 \mbox{ as } n \to \infty, \label{eq:var-lim}
  \end{equation}
  since the expected value of the estimator tends to $\theta$ and its
  variance goes to 0 when $n \to \infty$. Therefore, given any
  $\epsilon > 0$ and $\delta > 0$, we can find a sufficiently large
  $n$ and $k$ such that
  \[
    \Pr_{X}\left( \left| \hat{\theta}_{n,k}(X) - \theta \right| \geq
      \epsilon \right) \leq \frac{ \mbox{E}_{X}\left[
        (\hat{\theta}_{n,k}(X) - \theta)^2 \right] }{ \epsilon^2 } < \delta    
  \]
\end{proof}


\bibliographystyle{plainnat}
\bibliography{master-bibliography}
\end{document}
